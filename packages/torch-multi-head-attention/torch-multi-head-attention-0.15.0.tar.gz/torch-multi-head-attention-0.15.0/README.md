# PyTorch Multi-Head Attention

[![Travis](https://travis-ci.org/CyberZHG/torch-multi-head-attention.svg)](https://travis-ci.org/CyberZHG/torch-multi-head-attention)
[![Coverage](https://coveralls.io/repos/github/CyberZHG/torch-multi-head-attention/badge.svg?branch=master)](https://coveralls.io/github/CyberZHG/torch-multi-head-attention)

## Install

```bash
pip install torch-multi-head-attention
```

## Usage

```python
from torch_multi_head_attention import MultiHeadAttention

MultiHeadAttention(in_features=768, head_num=12)
```
