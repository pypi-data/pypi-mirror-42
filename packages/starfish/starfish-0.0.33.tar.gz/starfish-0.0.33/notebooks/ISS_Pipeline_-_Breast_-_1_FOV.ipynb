{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce In-situ Sequencing results with Starfish\n",
    "\n",
    "This notebook walks through a work flow that reproduces an ISS result for one field of view using the starfish package.\n",
    "\n",
    "## Load tiff stack and visualize one field of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from showit import image\n",
    "import pprint\n",
    "\n",
    "from starfish import data, FieldOfView\n",
    "from starfish.types import Features, Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_test_data = os.getenv(\"USE_TEST_DATA\") is not None\n",
    "experiment = data.ISS(use_test_data=use_test_data)\n",
    "\n",
    "\n",
    "# s.image.squeeze() simply converts the 4D tensor H*C*X*Y into a list of len(H*C) image planes for rendering by 'tile'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show input file format that specifies how the tiff stack is organized\n",
    "\n",
    "The stack contains multiple single plane images, one for each color channel, 'c', (columns in above image) and imaging round, 'r', (rows in above image). This protocol assumes that genes are encoded with a length 4 quatenary barcode that can be read out from the images. Each round encodes a position in the codeword. The maximum signal in each color channel (columns in the above image) corresponds to a letter in the codeword. The channels, in order, correspond to the letters: 'T', 'G', 'C', 'A'. The goal is now to process these image data into spatially organized barcodes, e.g., ACTG, which can then be mapped back to a codebook that specifies what gene this codeword corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "pp.pprint(experiment._src_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flat TIFF files are loaded into a 4-d tensor with dimensions corresponding to imaging round, channel, x, and y. For other volumetric approaches that image the z-plane, this would be a 5-d tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fov = experiment.fov()\n",
    "primary_image = fov[FieldOfView.PRIMARY_IMAGES]\n",
    "dots = fov['dots']\n",
    "nuclei = fov['nuclei']\n",
    "images = [primary_image, nuclei, dots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round, channel, x, y, z\n",
    "primary_image.xarray.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show auxiliary images captured during the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'dots' is a general stain for all possible transcripts. This image should correspond to the maximum projcection of all color channels within a single imaging round. This auxiliary image is useful for registering images from multiple imaging rounds to this reference image. We'll see an example of this further on in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dots_mp = dots.max_proj(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "dots_mp_numpy = dots._squeezed_numpy(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "image(dots_mp_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a DAPI auxiliary image, which specifically marks nuclei. This is useful cell segmentation later on in the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclei_mp = nuclei.max_proj(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "nuclei_mp_numpy = nuclei_mp._squeezed_numpy(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "image(nuclei_mp_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the codebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each 4 letter quatenary code (as read out from the 4 imaging rounds and 4 color channels) represents a gene. This relationship is stored in a codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.codebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter and scale raw data\n",
    "\n",
    "Now apply the white top hat filter to both the spots image and the individual channels. White top had enhances white spots on a black background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starfish.image import Filter\n",
    "\n",
    "# filter raw data\n",
    "masking_radius = 15\n",
    "filt = Filter.WhiteTophat(masking_radius, is_volume=False)\n",
    "for img in images:\n",
    "    filt.run(img, verbose=True, in_place=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each imaging round, the max projection across color channels should look like the dots stain.\n",
    "Below, this computes the max projection across the color channels of an imaging round and learns the linear transformation to maps the resulting image onto the dots image.\n",
    "\n",
    "The Fourier shift registration approach can be thought of as maximizing the cross-correlation of two images.\n",
    "\n",
    "In the below table, Error is the minimum mean-squared error, and shift reports changes in x and y dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starfish.image import Registration\n",
    "\n",
    "registration = Registration.FourierShiftRegistration(\n",
    "    upsampling=1000,\n",
    "    reference_stack=dots,\n",
    "    verbose=True)\n",
    "registered_image = registration.run(primary_image, in_place=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use spot-detector to create 'encoder' table  for standardized input  to decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pipeline exposes an encoder that translates an image into spots with intensities.  This approach uses a Gaussian spot detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starfish.spots import SpotFinder\n",
    "import warnings\n",
    "\n",
    "# parameters to define the allowable gaussian sizes (parameter space)\n",
    "min_sigma = 1\n",
    "max_sigma = 10\n",
    "num_sigma = 30\n",
    "threshold = 0.01\n",
    "\n",
    "p = SpotFinder.BlobDetector(\n",
    "    min_sigma=min_sigma,\n",
    "    max_sigma=max_sigma,\n",
    "    num_sigma=num_sigma,\n",
    "    threshold=threshold,\n",
    "    measurement_type='mean',\n",
    ")\n",
    "\n",
    "# detect triggers some numpy warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    # blobs = dots; define the spots in the dots image, but then find them again in the stack.\n",
    "    dots = dots.max_proj(Axes.ROUND, Axes.ZPLANE)\n",
    "    dots_numpy = dots._squeezed_numpy(Axes.ROUND, Axes.ZPLANE)\n",
    "    blobs_image = dots_numpy\n",
    "    intensities = p.run(registered_image, blobs_image=blobs_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The Encoder table is the hypothesized standardized file format for the output of a spot detector, and is the first output file format in the pipeline that is not an image or set of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`attributes` is produced by the encoder and contains all the information necessary to map the encoded spots back to the original image\n",
    "\n",
    "`x, y` describe the position, while `x_min` through `y_max` describe the bounding box for the spot, which is refined by a radius `r`. This table also stores the intensity and spot_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each assay type also exposes a decoder. A decoder translates each spot (spot_id) in the Encoder table into a gene (that matches a barcode) and associates this information with the stored position. The goal is to decode and output a quality score that describes the confidence in the decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are hard and soft decodings -- hard decoding is just looking for the max value in the code book. Soft decoding, by contrast, finds the closest code by distance (in intensity). Because different assays each have their own intensities and error modes, we leave decoders as user-defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = experiment.codebook.decode_per_round_max(intensities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to results from paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides house keeping genes, VIM and HER2 should be most highly expessed, which is consistent here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes, counts = np.unique(decoded.loc[decoded[Features.PASSES_THRESHOLDS]][Features.TARGET], return_counts=True)\n",
    "table = pd.Series(counts, index=genes).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling spots and decoding their gene information, cells must be segmented to assign genes to cells. This paper used a seeded watershed approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from starfish.image import Segmentation\n",
    "\n",
    "dapi_thresh = .16  # binary mask for cell (nuclear) locations\n",
    "stain_thresh = .22  # binary mask for overall cells // binarization of stain\n",
    "min_dist = 57\n",
    "\n",
    "registered_mp = registered_image.max_proj(Axes.CH, Axes.ZPLANE)\n",
    "registered_mp_numpy = registered_mp._squeezed_numpy(Axes.CH, Axes.ZPLANE)\n",
    "stain = np.mean(registered_mp_numpy, axis=0)\n",
    "stain = stain/stain.max()\n",
    "nuclei = nuclei.max_proj(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "nuclei_numpy = nuclei._squeezed_numpy(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "\n",
    "seg = Segmentation.Watershed(\n",
    "    nuclei_threshold=dapi_thresh,\n",
    "    input_threshold=stain_thresh,\n",
    "    min_distance=min_dist\n",
    ")\n",
    "label_image = seg.run(registered_image, nuclei)\n",
    "seg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results\n",
    "\n",
    "This FOV was selected to make sure that we can visualize the tumor/stroma boundary, below this is described by pseudo-coloring `HER2` (tumor) and vimentin (`VIM`, stroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "\n",
    "GENE1 = 'HER2'\n",
    "GENE2 = 'VIM'\n",
    "\n",
    "rgb = np.zeros(registered_image.tile_shape + (3,))\n",
    "nuclei_mp = nuclei.max_proj(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "nuclei_numpy = nuclei_mp._squeezed_numpy(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "rgb[:,:,0] = nuclei_numpy\n",
    "dots_mp = dots.max_proj(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "dots_mp_numpy = dots_mp._squeezed_numpy(Axes.ROUND, Axes.CH, Axes.ZPLANE)\n",
    "rgb[:,:,1] = dots_mp_numpy\n",
    "do = rgb2gray(rgb)\n",
    "do = do/(do.max())\n",
    "\n",
    "image(do,size=10)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore', FutureWarning)\n",
    "    is_gene1 = decoded.where(decoded[Features.AXIS][Features.TARGET] == GENE1, drop=True)\n",
    "    is_gene2 = decoded.where(decoded[Features.AXIS][Features.TARGET] == GENE2, drop=True)\n",
    "\n",
    "plt.plot(is_gene1.x, is_gene1.y, 'or')\n",
    "plt.plot(is_gene2.x, is_gene2.y, 'ob')\n",
    "plt.title(f'Red: {GENE1}, Blue: {GENE2}');"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}