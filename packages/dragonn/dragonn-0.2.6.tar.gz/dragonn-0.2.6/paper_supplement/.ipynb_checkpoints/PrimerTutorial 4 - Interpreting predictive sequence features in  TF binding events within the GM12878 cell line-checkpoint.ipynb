{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# naive overlap peaks that are not in IDR should be labeled as ambiguous, in addition to flanking bins\n",
    "# barplot of cases 1 -4 , all performance metrics\n",
    "# lower memory footprint for storing/reading hdf5 on Google Colab \n",
    "# Explain that hyperparameter search is performed online in similar way as Tut. 2. \n",
    "# Add calibration for the models. \n",
    "# Move the data zenodo. \n",
    "# Models in kipoi\n",
    "# use nans to indicate ambiguous labels \n",
    "# resample validation to match test set imbalance \n",
    "# add in tensorboard integration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train your DragoNN tutorial 4: \n",
    "## Interpreting predictive sequence features in  TF binding events within the GM12878 cell line. \n",
    "\n",
    "This tutorial is a supplement to the DragoNN manuscript and follows figure 8 in the manuscript. \n",
    "\n",
    "This tutorial will take 2 - 3 hours if executed on a GPU.\n",
    "\n",
    "## Outline<a name='outline'>\n",
    "<ol>\n",
    "    <li><a href=#1>Input data</a></li>\n",
    "    <li><a href=#2>Generating positive and negative bins for genome-wide training </a></li>\n",
    "    <li><a href=#3>Challenges of in vivo data : batch generators and class upsampling </a></li>\n",
    "    <li><a href=#3.5>Optional: Download pre-generated models and test-set predictions </a></li>\n",
    "    <li><a href=#4>Case 1: Negatives consist of shuffled references, single-tasked models</a></li>  \n",
    "    <li><a href=#5>Case 2: Whole-genome negatives, single-tasked models </a></li>\n",
    "    <li><a href=#6>Case 3: Whole-genome negatives, multi-tasked models </a></li>\n",
    "    <li><a href=#7>Case 4: What happens if we don't upsample positive examples in our batches? </a></li>\n",
    "    <li><a href=#8>Genome-wide interpretation of true positive predictions in SPI1, with DeepLIFT </a></li>\n",
    "    <li><a href=#9>Conclusions</a></li>    \n",
    "    <li><a href=#10>Save tutorial outputs</a></li>\n",
    "</ol>\n",
    "Github issues on the [dragonn repository](https://github.com/kundajelab/dragonn) with feedback, questions, and discussion are always welcome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the lines below if you are running this tutorial from Google Colab \n",
    "#!pip install dragonn>=0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure our results are reproducible\n",
    "from numpy.random import seed\n",
    "seed(1234)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/users/annashch/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#load dragonn tutorial utilities \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from dragonn.tutorial_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data <a name='1'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "Tutorials 1 - 3 have used simulated data generated with the simdna package. In this tutorial, we will examine how well CNN's are able to predict transcription factor binding for four TF's in vivo. \n",
    "\n",
    "We will learn to predict transcription factor binding for four transcription factors in the GM12878 cell line (one of the Tier 1 cell lines for the ENCODE project). First, we download the narrowPeak bed files for each of these transcription factors. You can skip the following code block if you already have the data downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CTCF, optimal IDR thresholded peaks, Stam Lab, hg19\n",
    "# https://www.encodeproject.org/experiments/ENCSR000DRZ/\n",
    "!wget -O CTCF.narrowPeak.gz http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/ctcf_ENCSR000DRZ/cromwell-executions/chip/e94d720e-08dd-42fa-bc77-2391e2b8c275/call-reproducibility_idr/execution/optimal_peak.regionPeak.gz \n",
    "\n",
    "## SPI1, optimal IDR thresholded peaks, Myers lab, hg19\n",
    "# https://www.encodeproject.org/experiments/ENCSR000BGQ/\n",
    "!wget -O SPI1.narrowPeak.gz http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/spi1_ENCSR000BGQ/cromwell-executions/chip/bb0c3c5a-3889-43fe-a218-05851cecc74a/call-reproducibility_idr/execution/optimal_peak.regionPeak.gz\n",
    "    \n",
    "## ZNF143, optimal IDR thresholded peaks, Snyder lab, hg19\n",
    "#https://www.encodeproject.org/experiments/ENCSR936XTK/\n",
    "!wget -O ZNF143.narrowPeak.gz http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/znf143_ENCSR936XTK/cromwell-executions/chip/8e1d0af8-3ca6-44e2-aff0-af83b2c0c061/call-reproducibility_idr/execution/optimal_peak.regionPeak.gz\n",
    "\n",
    "## SIX5, optimal IDR thresholded peaks, Myers Lab, hg19\n",
    "# https://www.encodeproject.org/experiments/ENCSR000BJE/\n",
    "!wget -O SIX5.narrowPeak.gz http://mitra.stanford.edu/kundaje/projects/dragonn/dragonn_gm12878_pipeline/six5_ENCSR00BJE/cromwell-executions/chip/bd36ae6c-a9e3-4d71-a938-c45ab86854fc/call-reproducibility_idr/execution/optimal_peak.regionPeak.gz\n",
    "\n",
    "## Download \"ambiguous\" peak sets -- these peaks are in the optimal overlap set across replicates, but are not\n",
    "## found to be reproducible at a high confidence (p<0.05) by IDR \n",
    "! wget -O CTCF.ambiguous.gz http://mitra.stanford.edu/kundaje/projects/dragonn/CTCF.ambiguous.gz\n",
    "! wget -O SPI1.ambiguous.gz http://mitra.stanford.edu/kundaje/projects/dragonn/SPI1.ambiguous.gz\n",
    "! wget -O ZNF143.ambiguous.gz http://mitra.stanford.edu/kundaje/projects/dragonn/ZNF143.ambiguous.gz\n",
    "! wget -O SIX5.ambiguous.gz http://mitra.stanford.edu/kundaje/projects/dragonn/SIX5.ambiguous.gz\n",
    "    \n",
    "## Download the hg19 chromsizes file (We only use chroms 1 -22, X, Y for training)\n",
    "#!wget https://github.com/kundajelab/dragonn/blob/keras_2.2_tensorflow_1.6_purekeras/paper_supplement/hg19.chrom.sizes\n",
    "    \n",
    "## Download the hg19 fasta reference genome (and corresponding .fai index)\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.gz\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/hg19.genome.fa.fai \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating positive and negative bins for genome-wide training <a name='2'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the *genomewide_labels* function from the  [seqdataloader](https://github.com/kundajelab/seqdataloader) package to generate positive and negative labels for the TF-ChIPseq peaks across the genome. We will treat each sample as a task for the model and compare the performance of the model on SPI1 task in the single-tasked and multi-tasked setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqdataloader import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTCF\tCTCF.narrowPeak.gz\t\tCTCF.ambiguous.gz\r\n",
      "SPI1\tSPI1.narrowPeak.gz\t\tSPI1.ambiguous.gz\r\n",
      "SIX5\tSIX5.narrowPeak.gz\t\tSIX5.ambiguous.gz\r\n",
      "ZNF143\tZNF143.narrowPeak.gz\t\tZNF143.ambiguous.gz\r\n"
     ]
    }
   ],
   "source": [
    "## seqdataloader accepts an input file, which we call tasks.tsv, with task names in column 1, the corresponding\n",
    "## peak files in column 2, skip column 3 (which will be used for regression in Tutorial 5), and ambiguous peaks in \n",
    "## column4 \n",
    "!echo \"CTCF\\tCTCF.narrowPeak.gz\\t\\tCTCF.ambiguous.gz\" > tasks.tsv \n",
    "!echo \"SPI1\\tSPI1.narrowPeak.gz\\t\\tSPI1.ambiguous.gz\" >> tasks.tsv \n",
    "!echo \"SIX5\\tSIX5.narrowPeak.gz\\t\\tSIX5.ambiguous.gz\" >> tasks.tsv \n",
    "!echo \"ZNF143\\tZNF143.narrowPeak.gz\\t\\tZNF143.ambiguous.gz\" >> tasks.tsv \n",
    "! cat tasks.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the parameter configuration below, seqdataloader splits the genome into 1kb regions, with a stride of 50. Each 1kb region is centered at a 200 bp bin, with a left flank of 400 bases and a right flank of 400 bases. \n",
    "\n",
    "* Each 200 bp bin is labeled as positive if a narrowPeak summit overlaps with it. \n",
    "\n",
    "* The bin is labeled ambiguous (label = -1) and excluded from training if there is some overlap with the narrowPeak, but the peak summit does not lie in that overlap. \n",
    "\n",
    "* The bin is labeled negative if there is no overlap with the narrowPeak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will include all chromosomes with the exception of 1,2, and 19 in our training set \n",
    "\n",
    "#1) Generate genome-wide negatives in addition to positives \n",
    "train_set_params={\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"TF.train.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':20,\n",
    "    'subthreads':2,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'    \n",
    "    }\n",
    "genomewide_labels(train_set_params)\n",
    "\n",
    "#2) Extract positive bins for each task for DeepBind training paradigm -- shuffled reference negatives to be \n",
    "#generated on the fly \n",
    "\n",
    "positives_train_set_params={\n",
    "    'store_positives_only':True,\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"positives.TF.train.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_exclude':['chr1','chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':20,\n",
    "    'subthreads':2,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(positives_train_set_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will include chromsome 1 in our validation set \n",
    "\n",
    "#1) Generate genome-wide negatives in addition to positives \n",
    "valid_set_params={'task_list':\"tasks.tsv\",\n",
    "    'outf':\"TF.valid.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':20,\n",
    "    'subthreads':2,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(valid_set_params)\n",
    "\n",
    "\n",
    "#2) Extract positive bins for each task for DeepBind training paradigm -- shuffled reference negatives to be \n",
    "#generated on the fly \n",
    "positives_valid_set_params={\n",
    "    'store_positives_only':True,\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"positives.TF.valid.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':'chr1',\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':20,\n",
    "    'subthreads':2,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(positives_valid_set_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will include chromosomes 2 and 19 in our testing set \n",
    "test_set_params={\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"TF.test.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':20,\n",
    "    'subthreads':2,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(test_set_params)\n",
    "\n",
    "#2) Extract positive bins for each task for DeepBind training paradigm -- shuffled reference negatives to be \n",
    "#generated on the fly \n",
    "positives_test_set_params={\n",
    "    'store_positives_only':True,\n",
    "    'task_list':\"tasks.tsv\",\n",
    "    'outf':\"positives.TF.test.hdf5\",\n",
    "    'output_type':'hdf5',\n",
    "    'chrom_sizes':'hg19.chrom.sizes',\n",
    "    'chroms_to_keep':['chr2','chr19'],\n",
    "    'bin_stride':50,\n",
    "    'left_flank':400,\n",
    "    'right_flank':400,\n",
    "    'bin_size':200,\n",
    "    'threads':20,\n",
    "    'subthreads':2,\n",
    "    'allow_ambiguous':True,\n",
    "    'labeling_approach':'peak_summit_in_bin_classification'\n",
    "    }\n",
    "genomewide_labels(positives_test_set_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the files that were generated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>CTCF</th>\n",
       "      <th>SPI1</th>\n",
       "      <th>SIX5</th>\n",
       "      <th>ZNF143</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHR</th>\n",
       "      <th>START</th>\n",
       "      <th>END</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">chr3</th>\n",
       "      <th>0</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <th>1050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <th>1100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <th>1150</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <th>1200</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <th>1250</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <th>1300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <th>1350</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <th>1400</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <th>1450</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 CTCF  SPI1  SIX5  ZNF143\n",
       "CHR  START END                           \n",
       "chr3 0     1000   0.0   0.0   0.0     0.0\n",
       "     50    1050   0.0   0.0   0.0     0.0\n",
       "     100   1100   0.0   0.0   0.0     0.0\n",
       "     150   1150   0.0   0.0   0.0     0.0\n",
       "     200   1200   0.0   0.0   0.0     0.0\n",
       "     250   1250   0.0   0.0   0.0     0.0\n",
       "     300   1300   0.0   0.0   0.0     0.0\n",
       "     350   1350   0.0   0.0   0.0     0.0\n",
       "     400   1400   0.0   0.0   0.0     0.0\n",
       "     450   1450   0.0   0.0   0.0     0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The code generates bed file outputs with a label of 1 or 0 for each 1kb\n",
    "# genome bin for each task. Note that the bins are shifted with a stride of 50.\n",
    "pd.read_hdf(\"TF.train.hdf5\",start=0,stop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>SPI1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHR</th>\n",
       "      <th>START</th>\n",
       "      <th>END</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">chr3</th>\n",
       "      <th>129600</th>\n",
       "      <th>130600</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129650</th>\n",
       "      <th>130650</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129700</th>\n",
       "      <th>130700</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129750</th>\n",
       "      <th>130750</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260300</th>\n",
       "      <th>261300</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260350</th>\n",
       "      <th>261350</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260400</th>\n",
       "      <th>261400</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260450</th>\n",
       "      <th>261450</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319100</th>\n",
       "      <th>320100</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319150</th>\n",
       "      <th>320150</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    SPI1\n",
       "CHR  START  END         \n",
       "chr3 129600 130600   1.0\n",
       "     129650 130650   1.0\n",
       "     129700 130700   1.0\n",
       "     129750 130750   1.0\n",
       "     260300 261300   1.0\n",
       "     260350 261350   1.0\n",
       "     260400 261400   1.0\n",
       "     260450 261450   1.0\n",
       "     319100 320100   1.0\n",
       "     319150 320150   1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When provided with the --store-positives_only flag, the code generates all bins for each task that are labeled positive.\n",
    "pd.read_hdf(\"SPI1.positives.TF.train.hdf5\",start=0,stop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>CTCF</th>\n",
       "      <th>SPI1</th>\n",
       "      <th>SIX5</th>\n",
       "      <th>ZNF143</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHR</th>\n",
       "      <th>START</th>\n",
       "      <th>END</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">chr2</th>\n",
       "      <th>0</th>\n",
       "      <th>1000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <th>1050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <th>1100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <th>1150</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <th>1200</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <th>1250</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <th>1300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <th>1350</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <th>1400</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <th>1450</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <th>1500</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <th>1550</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <th>1600</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <th>1650</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <th>1700</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <th>1750</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <th>1800</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <th>1850</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <th>1900</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <th>1950</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <th>2000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <th>2050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <th>2100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <th>2150</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <th>2200</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <th>2250</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <th>2300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <th>2350</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <th>2400</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <th>2450</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">chr19</th>\n",
       "      <th>59126500</th>\n",
       "      <th>59127500</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126550</th>\n",
       "      <th>59127550</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126600</th>\n",
       "      <th>59127600</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126650</th>\n",
       "      <th>59127650</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126700</th>\n",
       "      <th>59127700</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126750</th>\n",
       "      <th>59127750</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126800</th>\n",
       "      <th>59127800</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126850</th>\n",
       "      <th>59127850</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126900</th>\n",
       "      <th>59127900</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59126950</th>\n",
       "      <th>59127950</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127000</th>\n",
       "      <th>59128000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127050</th>\n",
       "      <th>59128050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127100</th>\n",
       "      <th>59128100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127150</th>\n",
       "      <th>59128150</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127200</th>\n",
       "      <th>59128200</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127250</th>\n",
       "      <th>59128250</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127300</th>\n",
       "      <th>59128300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127350</th>\n",
       "      <th>59128350</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127400</th>\n",
       "      <th>59128400</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127450</th>\n",
       "      <th>59128450</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127500</th>\n",
       "      <th>59128500</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127550</th>\n",
       "      <th>59128550</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127600</th>\n",
       "      <th>59128600</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127650</th>\n",
       "      <th>59128650</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127700</th>\n",
       "      <th>59128700</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127750</th>\n",
       "      <th>59128750</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127800</th>\n",
       "      <th>59128800</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127850</th>\n",
       "      <th>59128850</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127900</th>\n",
       "      <th>59128900</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59127950</th>\n",
       "      <th>59128950</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6046528 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         CTCF  SPI1  SIX5  ZNF143\n",
       "CHR   START    END                               \n",
       "chr2  0        1000       0.0   0.0   0.0     0.0\n",
       "      50       1050       0.0   0.0   0.0     0.0\n",
       "      100      1100       0.0   0.0   0.0     0.0\n",
       "      150      1150       0.0   0.0   0.0     0.0\n",
       "      200      1200       0.0   0.0   0.0     0.0\n",
       "      250      1250       0.0   0.0   0.0     0.0\n",
       "      300      1300       0.0   0.0   0.0     0.0\n",
       "      350      1350       0.0   0.0   0.0     0.0\n",
       "      400      1400       0.0   0.0   0.0     0.0\n",
       "      450      1450       0.0   0.0   0.0     0.0\n",
       "      500      1500       0.0   0.0   0.0     0.0\n",
       "      550      1550       0.0   0.0   0.0     0.0\n",
       "      600      1600       0.0   0.0   0.0     0.0\n",
       "      650      1650       0.0   0.0   0.0     0.0\n",
       "      700      1700       0.0   0.0   0.0     0.0\n",
       "      750      1750       0.0   0.0   0.0     0.0\n",
       "      800      1800       0.0   0.0   0.0     0.0\n",
       "      850      1850       0.0   0.0   0.0     0.0\n",
       "      900      1900       0.0   0.0   0.0     0.0\n",
       "      950      1950       0.0   0.0   0.0     0.0\n",
       "      1000     2000       0.0   0.0   0.0     0.0\n",
       "      1050     2050       0.0   0.0   0.0     0.0\n",
       "      1100     2100       0.0   0.0   0.0     0.0\n",
       "      1150     2150       0.0   0.0   0.0     0.0\n",
       "      1200     2200       0.0   0.0   0.0     0.0\n",
       "      1250     2250       0.0   0.0   0.0     0.0\n",
       "      1300     2300       0.0   0.0   0.0     0.0\n",
       "      1350     2350       0.0   0.0   0.0     0.0\n",
       "      1400     2400       0.0   0.0   0.0     0.0\n",
       "      1450     2450       0.0   0.0   0.0     0.0\n",
       "...                       ...   ...   ...     ...\n",
       "chr19 59126500 59127500   0.0   0.0   0.0     0.0\n",
       "      59126550 59127550   0.0   0.0   0.0     0.0\n",
       "      59126600 59127600   0.0   0.0   0.0     0.0\n",
       "      59126650 59127650   0.0   0.0   0.0     0.0\n",
       "      59126700 59127700   0.0   0.0   0.0     0.0\n",
       "      59126750 59127750   0.0   0.0   0.0     0.0\n",
       "      59126800 59127800   0.0   0.0   0.0     0.0\n",
       "      59126850 59127850   0.0   0.0   0.0     0.0\n",
       "      59126900 59127900   0.0   0.0   0.0     0.0\n",
       "      59126950 59127950   0.0   0.0   0.0     0.0\n",
       "      59127000 59128000   0.0   0.0   0.0     0.0\n",
       "      59127050 59128050   0.0   0.0   0.0     0.0\n",
       "      59127100 59128100   0.0   0.0   0.0     0.0\n",
       "      59127150 59128150   0.0   0.0   0.0     0.0\n",
       "      59127200 59128200   0.0   0.0   0.0     0.0\n",
       "      59127250 59128250   0.0   0.0   0.0     0.0\n",
       "      59127300 59128300   0.0   0.0   0.0     0.0\n",
       "      59127350 59128350   0.0   0.0   0.0     0.0\n",
       "      59127400 59128400   0.0   0.0   0.0     0.0\n",
       "      59127450 59128450   0.0   0.0   0.0     0.0\n",
       "      59127500 59128500   0.0   0.0   0.0     0.0\n",
       "      59127550 59128550   0.0   0.0   0.0     0.0\n",
       "      59127600 59128600   0.0   0.0   0.0     0.0\n",
       "      59127650 59128650   0.0   0.0   0.0     0.0\n",
       "      59127700 59128700   0.0   0.0   0.0     0.0\n",
       "      59127750 59128750   0.0   0.0   0.0     0.0\n",
       "      59127800 59128800   0.0   0.0   0.0     0.0\n",
       "      59127850 59128850   0.0   0.0   0.0     0.0\n",
       "      59127900 59128900   0.0   0.0   0.0     0.0\n",
       "      59127950 59128950   0.0   0.0   0.0     0.0\n",
       "\n",
       "[6046528 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We load our test set labels into memory here, as we will use them to measure performance in cases 1 - 4 below.\n",
    "#Note that we only load the labels into memory, not the actual test dataset. \n",
    "\n",
    "#It is not necessary to load the training/validation dataset or labels, see below. \n",
    "test_set=pd.read_hdf(\"TF.test.hdf5\")\n",
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges of in vivo data : batch generators and class upsampling <a name='3'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "In tutorials 1 - 3, we used the [keras fit](https://keras.io/models/sequential/#fit) function to train a CNN. However, when working with real data we face two new challenges: \n",
    "\n",
    "1) The dataset is much bigger. In our training set, there are 50,881,560 1kb bins, in our validation set, there are 4,984,994 bins, and in our test set there are 6,046,529 bins. Loading this dataset into memory to pass as a numpy array to the CNN code will require more memory than is available on many machines. Consequently, we use the [keras fit_generator](https://keras.io/models/sequential/#fit_generator) function to limit the memory footprint. This function reads in one batch of training and one batch of validation data at a time from a python generator. in *dragonn.generators*, we provide several python generator functions to match the scenarios below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.generators import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The dataset is highly imbalanced. Of the 50,881,559 1kb bins in the training set, only \n",
    "\n",
    "* 138,399 are labeled positive for the SPI1 task (367 negatives: 1 positive )\n",
    "\n",
    "* 131,033 are labeled positive for the CTCF task (388 negatives: 1 positive ) \n",
    "\n",
    "* 88,541 are labeled positive for the ZNF143 task (574 negatives: 1 positive ) \n",
    "\n",
    "* 15,630 are labeled positive for the SIX5 task (3255 negatives: 1 positive ) \n",
    "\n",
    "The class imbalance is far too high for the model to learn unassisted. Hence, we upsample the positive bins to include in each batch with the \"upsample\" argument to DataGenerator. The upsample argument accepts a fraction between 0 and 1 and ensures that this fraction of the batch consists of positive bins. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To prepare for model training, we import the necessary functions and submodules from keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta, SGD, RMSprop;\n",
    "import keras.losses;\n",
    "from keras.constraints import maxnorm;\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping, History, TensorBoard \n",
    "from keras import backend as K \n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "#we use a custom binary cross-entropy loss that can handle ambiguous labels (denoted with -1 ) and exclude them \n",
    "# from the loss calculation \n",
    "from dragonn.custom_losses import get_ambig_binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concise.metrics import tpr, tnr, fpr, fnr, precision, f1\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "def initialize_model(ntasks=1):\n",
    "    #Define the model architecture in keras (regularized, 3-layer convolution model followed by 1 dense layer)\n",
    "    model=Sequential() \n",
    "    \n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\", kernel_constraint=max_norm(7.0,axis=-1),input_shape=(1,1000,4)))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,15),padding=\"same\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(filters=50,kernel_size=(1,13),padding=\"same\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(1,40)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(ntasks))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "    #use the custom ambig_binary_crossentropy loss, indicating that a value of -1 indicates an ambiguous label \n",
    "    loss=get_ambig_binary_crossentropy(-1)\n",
    "    \n",
    "    ##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "    model.compile(optimizer='adam', loss=loss,\n",
    "                  metrics=[tpr,\n",
    "                           tnr,\n",
    "                           fpr,\n",
    "                           fnr,\n",
    "                           precision,\n",
    "                           f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pre-generated models and test set predictions (optional) <a name='3.5'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "#### **Note: on a GPU, each of the models below should take 10 - 15 minutes to train, with an additional 7-10 minutesu for prediction on the corresponding test sets. Although training the models is a valuable exercise, for the sake of time, we also provide hdf5 files with pre-trained models and test set predictions. These were generated by running the tutorial and saving the outputs at the end (see <a href=#10>Save tutorial outputs</a>). Uncomment the cell below if you wish to load the pre-trained models and test set predictions to use for performance comparison and DeepLIFT analyis** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the models \n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/case1_spi1_model.hdf5\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/case1_ctcf_model.hdf5\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/case2_spi1_model.hdf5\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/case2_ctcf_model.hdf5\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/case3_model.hdf5\n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/case4_spi1_model.hdf5\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the models from hdf5 \n",
    "#from keras.models import load_model \n",
    "#case1_spi1_model=load_model(\"case1_spi1_model.hdf5\")\n",
    "#case1_ctcf_model=load_model(\"case1_ctcf_model.hdf5\")\n",
    "#case2_spi1_model=load_model(\"case2_spi1_model.hdf5\")\n",
    "#case2_ctcf_model=load_model(\"case2_ctcf_model.hdf5\")\n",
    "#case3_model=load_model(\"case3_model.hdf5\")\n",
    "#case4_spi1_model=load_model(\"case4_spi1_model.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Download test set predictions \n",
    "#!wget http://mitra.stanford.edu/kundaje/projects/dragonn/test_set_predictions.hdf5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract predictions from hdf5 for each model \n",
    "#import h5py\n",
    "#test_set_predictions=h5py.File(\"test_set_predictions.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case1_spi1_test_predictions=test_set_predictions[\"case1_spi1_test_predictions\"].value\n",
    "#case1_ctcf_test_predictions=test_set_predictions[\"case1_spi1_test_predictions\"].value\n",
    "#case2_spi1_test_predictions=test_set_predictions[\"case2_spi1_test_predictions\"].value\n",
    "#case2_ctcf_test_predictions=test_set_predictions[\"case2_ctcf_test_predictions\"].value\n",
    "#case3_test_predictions=test_set_predictions[\"case3_test_predictions\"].value\n",
    "#case4_spi1_test_predictions=test_set_predictions[\"case4_spi1_test_predictions\"].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case1_spi1_test_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Negatives consist of shuffled references, single-tasked models<a name='4'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by training a model on the SPI1 and CTCF dataset with the following specifications: \n",
    "\n",
    "* We use dinucleotide-shuffled positive bins as the negative set. \n",
    "\n",
    "* Each batch contains one-hot encoded 1kb regions from the genome, as well as the one-hot-encoded reverse complement sequences of those regions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create generators for the training and validation data to meet these specifications: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators\n",
    "from dragonn.generators import * \n",
    "case1_spi1_train_gen=DataGenerator(\"SPI1.positives.TF.train.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False,batch_size=256)\n",
    "case1_spi1_valid_gen=DataGenerator(\"SPI1.positives.TF.valid.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False,batch_size=256)\n",
    "case1_ctcf_train_gen=DataGenerator(\"CTCF.positives.TF.train.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False,batch_size=256)\n",
    "case1_ctcf_valid_gen=DataGenerator(\"CTCF.positives.TF.valid.hdf5\",\"hg19.genome.fa.gz\",shuffled_ref_negatives=True,upsample=False,batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now follow the standard protocol we used in tutorials 1 - 3 to train a keras model, with the exception that we use the fit_generator function in keras, rather than the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you are running this notebook in google colab, uncomment the lines below to observe the model's training \n",
    "#!mkdir logs\n",
    "#%tensorboard --logdir logs \n",
    "#tensorboard_visualizer=TensorBoard(log_dir=\"logs\", histogram_freq=0, batch_size=500, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "#callbacks.append(tensorboard_visualizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "2156/2156 [==============================] - 94s 43ms/step - loss: 0.0321 - sensitivity: 0.9871 - specificity: 0.9902 - fpr: 0.0098 - fnr: 0.0129 - precision: 0.9898 - f1: 0.9883 - val_loss: 0.2462 - val_sensitivity: 0.8540 - val_specificity: 1.0000 - val_fpr: 3.0800e-05 - val_fnr: 0.1460 - val_precision: 1.0000 - val_f1: 0.9190\n",
      "Epoch 2/150\n",
      "2156/2156 [==============================] - 89s 41ms/step - loss: 0.0075 - sensitivity: 0.9974 - specificity: 0.9977 - fpr: 0.0023 - fnr: 0.0026 - precision: 0.9977 - f1: 0.9976 - val_loss: 0.0135 - val_sensitivity: 0.9976 - val_specificity: 0.9933 - val_fpr: 0.0067 - val_fnr: 0.0024 - val_precision: 0.9934 - val_f1: 0.9955\n",
      "Epoch 3/150\n",
      "2156/2156 [==============================] - 93s 43ms/step - loss: 0.0060 - sensitivity: 0.9979 - specificity: 0.9979 - fpr: 0.0021 - fnr: 0.0021 - precision: 0.9979 - f1: 0.9979 - val_loss: 0.0185 - val_sensitivity: 0.9889 - val_specificity: 0.9997 - val_fpr: 2.7720e-04 - val_fnr: 0.0111 - val_precision: 0.9997 - val_f1: 0.9942\n",
      "Epoch 4/150\n",
      "2156/2156 [==============================] - 95s 44ms/step - loss: 0.0045 - sensitivity: 0.9985 - specificity: 0.9984 - fpr: 0.0016 - fnr: 0.0015 - precision: 0.9984 - f1: 0.9984 - val_loss: 0.0126 - val_sensitivity: 0.9922 - val_specificity: 0.9992 - val_fpr: 7.6999e-04 - val_fnr: 0.0078 - val_precision: 0.9992 - val_f1: 0.9957\n",
      "Epoch 5/150\n",
      "2156/2156 [==============================] - 93s 43ms/step - loss: 0.0037 - sensitivity: 0.9988 - specificity: 0.9987 - fpr: 0.0013 - fnr: 0.0012 - precision: 0.9987 - f1: 0.9987 - val_loss: 0.0074 - val_sensitivity: 0.9962 - val_specificity: 0.9992 - val_fpr: 7.6999e-04 - val_fnr: 0.0038 - val_precision: 0.9992 - val_f1: 0.9977\n",
      "Epoch 6/150\n",
      "2156/2156 [==============================] - 97s 45ms/step - loss: 0.0034 - sensitivity: 0.9989 - specificity: 0.9988 - fpr: 0.0012 - fnr: 0.0011 - precision: 0.9988 - f1: 0.9989 - val_loss: 0.0087 - val_sensitivity: 0.9996 - val_specificity: 0.9938 - val_fpr: 0.0062 - val_fnr: 3.6959e-04 - val_precision: 0.9939 - val_f1: 0.9967\n",
      "Epoch 7/150\n",
      "2156/2156 [==============================] - 97s 45ms/step - loss: 0.0028 - sensitivity: 0.9992 - specificity: 0.9990 - fpr: 0.0010 - fnr: 8.2256e-04 - precision: 0.9990 - f1: 0.9991 - val_loss: 0.0077 - val_sensitivity: 0.9959 - val_specificity: 0.9996 - val_fpr: 3.6959e-04 - val_fnr: 0.0041 - val_precision: 0.9996 - val_f1: 0.9977\n",
      "Epoch 8/150\n",
      "2156/2156 [==============================] - 97s 45ms/step - loss: 0.0027 - sensitivity: 0.9992 - specificity: 0.9990 - fpr: 9.6750e-04 - fnr: 7.6096e-04 - precision: 0.9990 - f1: 0.9991 - val_loss: 0.0148 - val_sensitivity: 0.9911 - val_specificity: 0.9999 - val_fpr: 1.2320e-04 - val_fnr: 0.0089 - val_precision: 0.9999 - val_f1: 0.9955\n"
     ]
    }
   ],
   "source": [
    "#Train the SPI1 model \n",
    "case1_spi1_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case1_spi1=case1_spi1_model.fit_generator(case1_spi1_train_gen,\n",
    "                                                  validation_data=case1_spi1_valid_gen,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1842/2047 [=========================>....] - ETA: 8s - loss: 0.0413 - sensitivity: 0.9812 - specificity: 0.9896 - fpr: 0.0104 - fnr: 0.0188 - precision: 0.9879 - f1: 0.9835"
     ]
    }
   ],
   "source": [
    "#Train the CTCF model \n",
    "case1_ctcf_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case1_ctcf=case1_ctcf_model.fit_generator(case1_ctcf_train_gen,\n",
    "                                                  validation_data=case1_ctcf_valid_gen,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for SPI1  \n",
    "from dragonn.tutorial_utils import plot_learning_curve\n",
    "plot_learning_curve(history_case1_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curve for CTCF \n",
    "plot_learning_curve(history_case1_ctcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now measure how well the models performed by calculating performance metrics on the test splits across the whole genome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case1_spi1_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['SPI1'])\n",
    "case1_spi1_test_predictions=case1_spi1_model.predict_generator(case1_spi1_test_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case1_ctcf_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['CTCF'])\n",
    "case1_ctcf_test_predictions=case1_ctcf_model.predict_generator(case1_ctcf_test_gen,max_queue_size=5000, workers=40, use_multiprocessing=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format true & predicted test labels for performance assessment \n",
    "\n",
    "#if test_set.shape is not a multiple of batch_size, \n",
    "#there may be some extra values in test_set that need to get truncated.\n",
    "spi1_test_truth=np.expand_dims(test_set['SPI1'][0:case1_spi1_test_predictions.shape[0]],1).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "from dragonn.metrics import ClassificationResult\n",
    "print(ClassificationResult(spi1_test_truth,case1_spi1_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctcf_test_truth=np.expand_dims(test_set['CTCF'][0:case1_ctcf_test_predictions.shape[0]],1).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(ctcf_test_truth,case1_ctcf_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Whole-genome negatives, single-tasked models <a name='5'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "* Since the genomewide imbalance of negative bins to positive bins is very high, we upsample each batch in the training set to contain 30% positive samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators\n",
    "from dragonn.generators import * \n",
    "case2_spi1_train_gen=DataGenerator(\"TF.train.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"SPI1\"],upsample_ratio=0.3,batch_size=256)\n",
    "case2_spi1_valid_gen=DataGenerator(\"TF.valid.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"SPI1\"],upsample=False,batch_size=256)\n",
    "case2_ctcf_train_gen=DataGenerator(\"TF.train.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"CTCF\"],upsample_ratio=0.3,batch_size=256)\n",
    "case2_ctcf_valid_gen=DataGenerator(\"TF.valid.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"CTCF\"],upsample=False,batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 model \n",
    "case2_spi1_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case2_spi1=case2_spi1_model.fit_generator(case2_spi1_train_gen,\n",
    "                                                  validation_data=case2_spi1_valid_gen,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=5000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the CTCF model \n",
    "case2_ctcf_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case2_ctcf=case2_ctcf_model.fit_generator(case2_ctcf_train_gen,\n",
    "                                                  validation_data=case2_ctcf_valid_gen,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=5000,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for SPI1  \n",
    "plot_learning_curve(history_case2_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for CTCF  \n",
    "plot_learning_curve(history_case2_ctcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get model predictions on the test set \n",
    "case2_spi1_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['SPI1'])\n",
    "case2_spi1_test_predictions=case2_spi1_model.predict_generator(case2_spi1_test_gen,\n",
    "                                                               max_queue_size=5000, \n",
    "                                                               workers=40, \n",
    "                                                               use_multiprocessing=True, \n",
    "                                                               verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case2_ctcf_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['CTCF'])\n",
    "case2_ctcf_test_predictions=case2_ctcf_model.predict_generator(case2_ctcf_test_gen,max_queue_size=5000, workers=40, use_multiprocessing=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.metrics import ClassificationResult\n",
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(spi1_test_truth,case2_spi1_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(ctcf_test_truth,case2_ctcf_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Whole-genome negatives, multi-tasked models <a name='6'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators for multi-tasked models. Guarantee 10% positives in each batch \n",
    "case3_train_gen=DataGenerator(\"TF.train.hdf5\",\"hg19.genome.fa.gz\",upsample_ratio=0.3,batch_size=256)\n",
    "case3_valid_gen=DataGenerator(\"TF.valid.hdf5\",\"hg19.genome.fa.gz\",upsample=False,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 model \n",
    "case3_model=initialize_model(4)\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case3=case3_model.fit_generator(case3_train_gen,\n",
    "                                        validation_data=case3_valid_gen,\n",
    "                                        steps_per_epoch=10000,\n",
    "                                        validation_steps=5000,\n",
    "                                        epochs=150,\n",
    "                                        verbose=1,\n",
    "                                        use_multiprocessing=True,\n",
    "                                        workers=40,\n",
    "                                        max_queue_size=100,\n",
    "                                        callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for the multi-tasked model   \n",
    "plot_learning_curve(history_case3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case3_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                             \"hg19.genome.fa.gz\",\n",
    "                             upsample=False,\n",
    "                             add_revcomp=False,\n",
    "                             batch_size=1000)\n",
    "case3_test_predictions=case3_model.predict_generator(case3_test_gen,\n",
    "                                                     max_queue_size=5000, \n",
    "                                                     workers=40, \n",
    "                                                     use_multiprocessing=True, \n",
    "                                                     verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_truth=test_set[0:case3_test_predictions.shape[0]].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_truth.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(np.expand_dims(test_truth['SPI1'],1),\n",
    "                           np.expand_dims(case3_test_predictions[:,0],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(np.expand_dims(test_truth['CTCF'],1),\n",
    "                           np.expand_dims(case3_test_predictions[:,1],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(np.expand_dims(test_truth['ZNF143'],1),\n",
    "                           np.expand_dims(case3_test_predictions[:,2],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(np.expand_dims(test_truth['SIX5'],1),\n",
    "                           np.expand_dims(case3_test_predictions[:,3],1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: What happens if we don't upsample positive examples in our batches? <a name='7'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generators\n",
    "case4_spi1_train_gen=DataGenerator(\"TF.train.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"SPI1\"],upsample=False,batch_size=256)\n",
    "case4_spi1_valid_gen=DataGenerator(\"TF.valid.hdf5\",\"hg19.genome.fa.gz\",tasks=[\"SPI1\"],upsample=False,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the SPI1 model \n",
    "case4_spi1_model=initialize_model()\n",
    "\n",
    "## use the keras fit_generator function to train the model with early stopping after 3 epochs \n",
    "history_case4_spi1=case4_spi1_model.fit_generator(case4_spi1_train_gen,\n",
    "                                                  validation_data=case4_spi1_valid_gen,\n",
    "                                                  steps_per_epoch=10000,\n",
    "                                                  validation_steps=5000,\n",
    "                                                  epochs=150,\n",
    "                                                  verbose=1,\n",
    "                                                  use_multiprocessing=True,\n",
    "                                                  workers=40,\n",
    "                                                  max_queue_size=100,\n",
    "                                                  callbacks=[EarlyStopping(patience=3,restore_best_weights=True),History()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the learning curves for SPI1  \n",
    "plot_learning_curve(history_case4_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We  use a custom batch_predict function to generate predictions on the test set, one batch at a time \n",
    "case4_spi1_test_gen=DataGenerator(\"TF.test.hdf5\",\n",
    "                                   \"hg19.genome.fa.gz\",\n",
    "                                     upsample=False,\n",
    "                                     add_revcomp=False,\n",
    "                                     batch_size=1000,\n",
    "                                     tasks=['SPI1'])\n",
    "case4_spi1_test_predictions=case4_spi1_model.predict_generator(case4_spi1_test_gen,max_queue_size=5000, workers=40, use_multiprocessing=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Generate a ClassificationResult object to print performance metrics on held-out test set \n",
    "print(ClassificationResult(spi1_test_truth,case4_spi1_test_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome-wide interpretation of true positive predictions in SPI1, with DeepLIFT <a name='8'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "The highest test set auPRC was observed in Case 2 -- the single-tasked, genome-wide, SPI1 model with upsampling of positives. Let's run DeepLIFT on some high-confidence true positives. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the true positive predictions with a threshold of 0.9 (i.e. high confidence true positive predictions)\n",
    "true_pos_spi1=test_truth[spi1_test_truth*case2_spi1_test_predictions >0.9]\n",
    "true_pos_spi1.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_spi1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.utils import one_hot_from_bed\n",
    "deep_lift_input_spi1=one_hot_from_bed([i for i in true_pos_spi1.index],\"hg19.genome.fa.gz\")\n",
    "deep_lift_input_spi1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import deeplift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift_scores_spi1=deeplift(case2_spi1_model,deep_lift_input_spi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift_scores_spi1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a few of the DeepLIFT tracks and see if the model successfully learned SPI1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dragonn.tutorial_utils import  plot_seq_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[0],deep_lift_input_spi1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[1],deep_lift_input_spi1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2],deep_lift_input_spi1[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's zoom in to the center of one sequence so that it is easier to distinguish the motif: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_spi1[2].squeeze()[450:550],deep_lift_input_spi1[2].squeeze()[450:550])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we query the sequence \"TCACTTCCCCTT\" in the [TomTom](http://meme-suite.org/tools/tomtom) software from the MEME suite, we find that the motif is a good match for SPIB (p-value = 8.93e-7): \n",
    "<img src=\"tutorial_images/SPI1.Tut4.png\" alt=\"SPI1TomTom\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret the high-confidence true positives from the CTCF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the true positive predictions with a threshold of 0.9 (i.e. high confidence predictions)\n",
    "true_pos_ctcf=test_truth[ctcf_test_truth*case2_ctcf_test_predictions >0.9]\n",
    "true_pos_ctcf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift_input_ctcf=one_hot_from_bed([i for i in true_pos_ctcf.index],\"hg19.genome.fa.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lift_scores_ctcf=deeplift(case2_ctcf_model,deep_lift_input_ctcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_ctcf[0],deep_lift_input_ctcf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_ctcf[1],deep_lift_input_ctcf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_ctcf[2],deep_lift_input_ctcf[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_seq_importance(deep_lift_scores_ctcf[0].squeeze()[500:600],deep_lift_input_ctcf[0].squeeze()[500:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying the sequence \"GCGCCCTCTGCTGG\" in TomTom confirms a strong match (p=1.43e-6) to the canonical CTCF motif:\n",
    "<img src=\"tutorial_images/CTCF.Tut4.png\" alt=\"CTCFTomTom\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a name='9'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our analysis of *in vivo* transcription factor ChiP-seq datasets, we draw the following conclusions: \n",
    "\n",
    "* Training the models genome-wide by splitting the genome into short (200 bp), overlapping (stride=50) bins leads to improved generalization performance on the test set as compared to \"easier\" negative sets. I.E we observed superior test set performance for both the SPI1 and CTCF tasks when training on the whole genome as compared to using shuffled reference negatives. \n",
    "\n",
    "\n",
    "* Due to high class imbalance in *in vivo* data, it is necessary to upsample positive examples in each batch to facilitate model training. We found that upsampling positives to constitute 30% of each batch worked well --i.e. interpretation with DeepLIFT indicates the model learned the correct motif in CTCF and SPI1 ChIP-seq datasets. What happens if you alter the *upsample_ratio* parameter in the generator function? Does the model learn equally well with *upsample_ratio=0.1*? Does it learn better or worse with *upsample_ratio=0.5*? \n",
    "\n",
    "\n",
    "* Multi-tasking can lead to improved performance for tasks that are similar (i.e. CTCF/ZNF143/SIX5 are similar motifs). However, multi-tasking does not improve performance on tasks that are different (i.e. the SPI1 motif does not resemble the other three motifs, so multi-tasking does not improve performance for the SPI1 task). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save tutorial outputs <a name='10'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "We save the models and test set predictions generated in this tutorial to an hdf5 file so that they can be loaded more readily in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the models \n",
    "case1_spi1_model.save(\"case1_spi1_model.hdf5\")\n",
    "case1_ctcf_model.save(\"case1_ctcf_model.hdf5\")\n",
    "case2_spi1_model.save(\"case2_spi1_model.hdf5\")\n",
    "case2_ctcf_model.save(\"case2_ctcf_model.hdf5\")\n",
    "case3_model.save(\"case3_model.hdf5\")\n",
    "case4_spi1_model.save(\"case4_spi1_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the test set predictions \n",
    "import h5py \n",
    "test_set_predictions=h5py.File(\"test_set_predictions.hdf5\",'w')\n",
    "test_set_predictions.create_dataset(\"case1_spi1_test_predictions\",data=case1_spi1_test_predictions)\n",
    "test_set_predictions.create_dataset(\"case1_ctcf_test_predictions\",data=case1_ctcf_test_predictions)\n",
    "test_set_predictions.create_dataset(\"case2_spi1_test_predictions\",data=case2_spi1_test_predictions)\n",
    "test_set_predictions.create_dataset(\"case2_ctcf_test_predictions\",data=case2_ctcf_test_predictions)\n",
    "test_set_predictions.create_dataset(\"case3_test_predictions\",data=case3_test_predictions)\n",
    "test_set_predictions.create_dataset(\"case4_spi1_test_predictions\",data=case4_spi1_test_predictions)\n",
    "test_set_predictions.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
