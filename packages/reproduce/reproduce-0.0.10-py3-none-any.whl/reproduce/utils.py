"""utility functions that are useful for Reproduce."""
import os
import re
import logging
import shutil
import hashlib
import base64

import urllib.request
import crcmod
import google.cloud.client
import google.cloud.storage

LOGGER = logging.getLogger(__name__)
HASH_ALGORITHM = 'blake2b'
MAX_HASH_DIGEST_LENGTH = 32


def url_fetcher(url, path):
    """Download `url` to `path`."""
    LOGGER.info(f'fetching {path}')
    response = urllib.request.urlopen(url)
    with open(path, 'wb') as out_file:
        shutil.copyfileobj(response, out_file)
    response.close()


def upload_to_google_bucket(
        file_path_list, bucket_id, iam_token_path):
    """Hash & rename an upload files to a google bucket.

    Parameters:
        file_path_list (list): list of file paths to upload, note directories
            in each path will be dropped when naming the Google blob, thus
            an exception will be raised if there is a duplicate filename in
            this list.
        bucket_id (str): name of a google bucket that is accessible
            by the IAM token.
        iam_token_path (str): path to a google IAM json file that was
            generated by https://cloud.google.com/iam/docs/creating-managing-service-account-keys
            that has access to the named bucket `bucket_id`.

    Returns:
        a dict that matches filenames to (bucket_id, blob_id) tuples.
    """
    # check for duplicate filenames
    bucket_id_to_filename_map = {}
    duplicate_filenames = []
    filename_list = []
    for path in file_path_list:
        filename = os.path.basename(path)
        if filename not in bucket_id_to_filename_map:
            filename_list.append(filename)
            bucket_id_to_filename_map[filename] = None
        else:
            duplicate_filenames.append(path)
    if duplicate_filenames:
        raise ValueError(
            f"`file_path_list` has duplicate filenames, must contain only "
            " unique filenames. Duplicates are: {duplicate_filenames}")

    # upload a file at a time
    client = google.cloud.storage.client.Client.from_service_account_json(
        iam_token_path)
    bucket = client.get_bucket(bucket_id)
    for file_path in file_path_list:
        filename = os.path.basename(file_path)
        # calculate a local checksum
        LOGGER.info(f'calculating crc-32 checksum for {filename}')
        file_hash, local_crc32c_digest = hash_and_checkum_file(
            file_path, HASH_ALGORITHM, buf_size=2**24)
        LOGGER.info(
            f'crc-32 is {local_crc32c_digest}, {HASH_ALGORITHM} is '
            f'{file_hash} for {filename}')
        # blob id will be the filename with ah {HASH_ALG}_{HASH} appended
        # before the extension.
        file_pre, file_ext = os.path.splitext(filename)
        blob_id = f'{file_pre}_{HASH_ALGORITHM}_{file_hash}{file_ext}'
        # limit chunk size so we don't try to load the entire thing into memory
        blob = bucket.blob(blob_id, chunk_size=2**24)
        if blob.exists():
            # validate that the uploaded blob has the same crc
            blob.update()
            if local_crc32c_digest == blob.crc32c:
                LOGGER.info(
                    f'blob {blob_id} exists and crcs ({local_crc32c_digest}) '
                    f'match for {blob.name}')
                bucket_id_to_filename_map[filename] = (bucket_id, blob_id)
                continue
            else:
                raise RuntimeError(
                    "blob exists and crcs (local: %s, remote: %s) do not "
                    "match for %s", local_crc32c_digest, blob.crc32c,
                    blob.name)
            LOGGER.info(
                "crc blob (%s) local crc (%s) don't match",
                blob.crc32c, local_crc32c_digest)

        blob.crc32c = local_crc32c_digest
        LOGGER.info(f'uploading {filename} to blob {blob_id}')
        blob.upload_from_filename(file_path)
        LOGGER.info(f'successful upload of blob {blob_id}')
        bucket_id_to_filename_map[filename] = (bucket_id, blob_id)
    return bucket_id_to_filename_map


def hash_and_checkum_file(file_path, hash_algorithm, buf_size=2**20):
    """Return a hex and crc32 digest of `file_path`.

    Parameters:
        file_path (string): path to file to hash.
        hash_algorithm (string): a hash function id that exists in
            hashlib.algorithms_available.
        buf_size (int): number of bytes to read from `file_path` at a time
            for digesting.

    Returns:
        a (hash, crc32) hex digest tuple with hash algorithm `hash_algorithm`
        of the binary contents of `file_path` and the crc32 checksum of that
        file.

    """
    crc32c = crcmod.predefined.Crc('crc-32c')
    hash_func = hashlib.new(hash_algorithm)
    with open(file_path, 'rb') as f:
        binary_data = f.read(buf_size)
        while binary_data:
            hash_func.update(binary_data)
            crc32c.update(binary_data)
            binary_data = f.read(buf_size)
    # We return the hash and CRC32 checksum in hexadecimal format
    return (
        hash_func.hexdigest()[:MAX_HASH_DIGEST_LENGTH], base64.b64encode(
            crc32c.digest()).decode('utf-8'))


def valid_hash(file_path, expected_hash, buf_size=2**20):
    """Validate that the file at `file_path` matches `expected_hash`.

    Parameters:
        file_path (str): path to file location on disk.
        expected_hash (str or tuple): if a tuple, a "hash algorithm",
            "expected_hash" pair that will be used to hash `expected_path`
            and confirm that the hash of that file is equivalent to the
            expected hash value. If the `expected_path` does not
            match the hash, this function will raise an AssertionError.

            Otherwise must be the value "embedded" which attempts to parse
            `file_path` for the pattern
            filename_{hash_algorthm}_{hash_value}.{rest of filename}.
        buf_size (int): (optional) number of bytes to read from `file_path`
            at a time for digesting.

    Returns:
        True if `file_path` hashes to `expected_hash`.

    Raises:
        ValueError if `expected_hash == 'embedded'` and `file_path` does not
        match the appropriate file pattern.

        IOError if `file_path` not found.

    """
    if not os.path.exists(file_path):
        raise IOError(f'{file_path} not found.')
    if isinstance(expected_hash, tuple):
        hash_algorithm = expected_hash[0]
        expected_hash_value = expected_hash[1]
    elif expected_hash == 'embedded':
        hash_re_pattern = r'.*_([^_]+)_([0-9a-f]+)\.[^_]*$'
        hash_match = re.match(hash_re_pattern, file_path)
        if not hash_match:
            raise ValueError(
                f"file_path: {file_path} did not end "
                "in an [hash_alg]_[hexhash][.ext] format")
        hash_algorithm = hash_match.group(1)
        expected_hash_value = hash_match.group(2)
    else:
        raise ValueError(
            "Invalid value for `expected_hash`, expecting either a tuple "
            "or 'embedded': {expected_hash}")

    actual_hash, _ = hash_and_checkum_file(file_path, hash_algorithm)
    return expected_hash_value == actual_hash


def google_bucket_fetch(
        bucket_id, blob_id, iam_token_path, target_path):
    """Create a function to download a Google Blob to a given path.

    Parameters:
        url (string): url to blob, matches the form
            '^https://storage.cloud.google.com/([^/]*)/(.*)$'
        iam_token_path (string): path to Google Cloud private key generated by
        https://cloud.google.com/iam/docs/creating-managing-service-account-keys
        target_path (string): path to target file.

    Returns:
        a function with a single `path` argument to the target file. Invoking
            this function will download the Blob to `path`.

    """
    client = google.cloud.storage.client.Client.from_service_account_json(
        iam_token_path)
    bucket = client.get_bucket(bucket_id)
    blob = google.cloud.storage.Blob(blob_id, bucket, chunk_size=2**24)
    LOGGER.info(f'downloading blob {target_path} from {bucket_id}:{blob_id}')
    try:
        os.makedirs(os.path.dirname(target_path))
    except os.error:
        pass
    blob.download_to_filename(target_path)


def google_bucket_fetch_and_validate(
        gs_path, iam_token_path, target_path):
    """Download a Google Blob to a given path and hash.

    Raises an exception if the downloaded file does not match its embedded
    hash.

    Parameters:
        gs_path (string): gs:// format to blob, matches the form
            '^gs://[bucket]/[blob]$'
        iam_token_path (string): path to Google Cloud private key generated by
        https://cloud.google.com/iam/docs/creating-managing-service-account-keys
        target_path (string): path to target file.

    Raises:
        ValueError if downloaded file does not match its embedded fingerprint
            where the filename is of the form
            [filename]_[hash_alg]_[fingerprint].ext

    Returns:
        None.

    """
    bucket_id, blob_id = re.match('^gs://([^/]+)/(.*)$', gs_path).groups()
    google_bucket_fetch(
        bucket_id, blob_id, iam_token_path, target_path)
    if not valid_hash(target_path, 'embedded'):
        raise ValueError(f"{target_path}' does not match its expected hash")


def url_fetch_and_validate(url, target_path):
    """Download a Google Blob to a given path and hash.

    Parameters:
        url (string): url to a file to fetch.
        target_path (string): path to download the file into, must match
            an embedded hash/algorithm pair.

    Raises:
        ValueError if downloaded file does not match its embedded fingerprint
            where the filename is of the form
            [filename]_[hash_alg]_[fingerprint].ext

    Returns:
        None.

    """
    url_fetcher(url, target_path)
    if not valid_hash(target_path, 'embedded'):
        raise ValueError(f"{target_path}' does not match its expected hash")
