Metadata-Version: 2.0
Name: harvesttext
Version: 0.5.4
Summary: UNKNOWN
Home-page: https://github.com/blmoistawinde/HarvestText
Author: blmoistawinde
Author-email: 1840962220@qq.com
License: MIT
Description-Content-Type: text/markdown
Keywords: NLP,tokenizing,entity linking,sentiment analysis
Platform: all
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Natural Language :: Chinese (Simplified)
Classifier: Natural Language :: Chinese (Traditional)
Classifier: Programming Language :: Python :: 3
Classifier: Topic :: Text Processing
Requires-Dist: jieba
Requires-Dist: networkx
Requires-Dist: numpy
Requires-Dist: pandas
Requires-Dist: pyhanlp
Requires-Dist: pypinyin
Requires-Dist: pyxDamerauLevenshtein (==1.5)
Requires-Dist: rdflib

# HarvestText

Sow with little data seed, harvest much from a text field.

æ’­æ’’å‡ å¤šç§å­è¯ï¼Œæ”¶è·ä¸‡åƒé¢†åŸŸå®

![PyPI - Python Version](https://img.shields.io/badge/python-3.6-blue.svg) ![GitHub](https://img.shields.io/github/license/mashape/apistatus.svg) ![Version](https://img.shields.io/badge/version-V0.5-red.svg)

## ç”¨é€”
HarvestTextæ˜¯ä¸€ä¸ªä¸“æ³¨æ— ï¼ˆå¼±ï¼‰ç›‘ç£æ–¹æ³•ï¼Œèƒ½å¤Ÿæ•´åˆé¢†åŸŸçŸ¥è¯†ï¼ˆå¦‚ç±»å‹ï¼Œåˆ«åï¼‰å¯¹ç‰¹å®šé¢†åŸŸæ–‡æœ¬è¿›è¡Œç®€å•é«˜æ•ˆåœ°å¤„ç†å’Œåˆ†æçš„åº“ã€‚

å…·ä½“åŠŸèƒ½å¦‚ä¸‹ï¼š

<a id="ç›®å½•">ç›®å½•:</a>
- åŸºæœ¬å¤„ç†
	- [ç²¾ç»†åˆ†è¯åˆ†å¥](#å®ä½“é“¾æ¥)
		- å¯åŒ…å«æŒ‡å®šè¯å’Œç±»åˆ«çš„åˆ†è¯ã€‚å……åˆ†è€ƒè™‘çœç•¥å·ï¼ŒåŒå¼•å·ç­‰ç‰¹æ®Šæ ‡ç‚¹çš„åˆ†å¥ã€‚
	- [å®ä½“é“¾æ¥](#å®ä½“é“¾æ¥)
		- æŠŠåˆ«åï¼Œç¼©å†™ä¸ä»–ä»¬çš„æ ‡å‡†åè”ç³»èµ·æ¥ã€‚ 
	- [å‘½åå®ä½“è¯†åˆ«](#å‘½åå®ä½“è¯†åˆ«)
		- æ‰¾åˆ°ä¸€å¥å¥å­ä¸­çš„äººåï¼Œåœ°åï¼Œæœºæ„åç­‰å‘½åå®ä½“ã€‚
	- [ä¾å­˜å¥æ³•åˆ†æ](#ä¾å­˜å¥æ³•åˆ†æ)
		- åˆ†æè¯­å¥ä¸­å„ä¸ªè¯è¯­ï¼ˆåŒ…æ‹¬é“¾æ¥åˆ°çš„å®ä½“ï¼‰çš„ä¸»è°“å®¾è¯­ä¿®é¥°ç­‰è¯­æ³•å…³ç³»ï¼Œ
	- [å†…ç½®èµ„æº](#å†…ç½®èµ„æº)
		- é€šç”¨åœç”¨è¯ï¼Œé€šç”¨æƒ…æ„Ÿè¯ï¼ŒITã€è´¢ç»ã€é¥®é£Ÿã€æ³•å¾‹ç­‰é¢†åŸŸè¯å…¸ã€‚å¯ç›´æ¥ç”¨äºä»¥ä¸Šä»»åŠ¡ã€‚
	- [ä¿¡æ¯æ£€ç´¢](#ä¿¡æ¯æ£€ç´¢)
		- ç»Ÿè®¡ç‰¹å®šå®ä½“å‡ºç°çš„ä½ç½®ï¼Œæ¬¡æ•°ç­‰ã€‚
	- [æ–°è¯å‘ç°](#æ–°è¯å‘ç°)
		- åˆ©ç”¨ç»Ÿè®¡è§„å¾‹ï¼ˆæˆ–è§„åˆ™ï¼‰å‘ç°è¯­æ–™ä¸­å¯èƒ½ä¼šè¢«ä¼ ç»Ÿåˆ†è¯é—æ¼çš„ç‰¹æ®Šè¯æ±‡ã€‚ä¹Ÿä¾¿äºä»æ–‡æœ¬ä¸­å¿«é€Ÿç­›é€‰å‡ºå…³é”®è¯ã€‚
	- [å­—ç¬¦æ‹¼éŸ³çº é”™](#å­—ç¬¦æ‹¼éŸ³çº é”™)
		- æŠŠè¯­å¥ä¸­æœ‰å¯èƒ½æ˜¯å·²çŸ¥å®ä½“çš„é”™è¯¯æ‹¼å†™ï¼ˆè¯¯å·®ä¸€ä¸ªå­—ç¬¦æˆ–æ‹¼éŸ³ï¼‰çš„è¯è¯­é“¾æ¥åˆ°å¯¹åº”å®ä½“ã€‚
	- [å­˜å–æ¶ˆé™¤](#å­˜å–ä¸æ¶ˆé™¤)
		- å¯ä»¥æœ¬åœ°ä¿å­˜æ¨¡å‹å†è¯»å–å¤ç”¨ï¼Œä¹Ÿå¯ä»¥æ¶ˆé™¤å½“å‰æ¨¡å‹çš„è®°å½•ã€‚
- é«˜å±‚åº”ç”¨
	- [æƒ…æ„Ÿåˆ†æ](#æƒ…æ„Ÿåˆ†æ)
		- ç»™å‡ºå°‘é‡ç§å­è¯ï¼ˆé€šç”¨çš„è¤’è´¬ä¹‰è¯è¯­ï¼‰ï¼Œå¾—åˆ°è¯­æ–™ä¸­å„ä¸ªè¯è¯­å’Œè¯­æ®µçš„è¤’è´¬åº¦ã€‚
	- [å…³ç³»ç½‘ç»œ](#å…³ç³»ç½‘ç»œ)
		- åˆ©ç”¨å…±ç°å…³ç³»ï¼Œè·å¾—å…³é”®è¯ä¹‹é—´çš„ç½‘ç»œã€‚æˆ–è€…ä»¥ä¸€ä¸ªç»™å®šè¯è¯­ä¸ºä¸­å¿ƒï¼Œæ¢ç´¢ä¸å…¶ç›¸å…³çš„è¯è¯­ç½‘ç»œã€‚
	- [æ–‡æœ¬æ‘˜è¦](#æ–‡æœ¬æ‘˜è¦)
		- åŸºäºTextrankç®—æ³•ï¼Œå¾—åˆ°ä¸€ç³»åˆ—å¥å­ä¸­çš„ä»£è¡¨æ€§å¥å­ã€‚
	- [äº‹å®æŠ½å–](#ä¾å­˜å¥æ³•åˆ†æ)
		- åˆ©ç”¨å¥æ³•åˆ†æï¼Œæå–å¯èƒ½è¡¨ç¤ºäº‹ä»¶çš„ä¸‰å…ƒç»„ã€‚
	- [ç®€æ˜“é—®ç­”ç³»ç»Ÿ](#ç®€æ˜“é—®ç­”ç³»ç»Ÿ)
		- ä»ä¸‰å…ƒç»„ä¸­å»ºç«‹çŸ¥è¯†å›¾è°±å¹¶åº”ç”¨äºé—®ç­”ï¼Œå¯ä»¥å®šåˆ¶ä¸€äº›é—®é¢˜æ¨¡æ¿ã€‚æ•ˆæœæœ‰å¾…æå‡ï¼Œä»…ä½œä¸ºç¤ºä¾‹ã€‚


ä½¿ç”¨æ¡ˆä¾‹:
- [ã€Šä¸‰å›½æ¼”ä¹‰ã€‹ä¸­çš„ç¤¾äº¤ç½‘ç»œ](https://blog.csdn.net/blmoistawinde/article/details/85344906)ï¼ˆå®ä½“åˆ†è¯ï¼Œæ–‡æœ¬æ‘˜è¦ï¼Œå…³ç³»ç½‘ç»œç­‰ï¼‰
- [2018ä¸­è¶…èˆ†æƒ…å±•ç¤ºç³»ç»Ÿ](https://blmoistawinde.github.io/SuperLegal2018Display/index.html)ï¼ˆå®ä½“åˆ†è¯ï¼Œæƒ…æ„Ÿåˆ†æï¼Œæ–°è¯å‘ç°\[è¾…åŠ©ç»°å·è¯†åˆ«\]ç­‰ï¼‰
- [è¿‘ä»£å²çº²è¦ä¿¡æ¯æŠ½å–åŠé—®ç­”ç³»ç»Ÿ](https://blog.csdn.net/blmoistawinde/article/details/86557070)(å‘½åå®ä½“è¯†åˆ«ï¼Œä¾å­˜å¥æ³•åˆ†æï¼Œç®€æ˜“é—®ç­”ç³»ç»Ÿ)


## ç”¨æ³•


é¦–å…ˆå®‰è£…ï¼Œ
ä½¿ç”¨pip
```
pip install harvesttext
```

æˆ–è¿›å…¥setup.pyæ‰€åœ¨ç›®å½•ï¼Œç„¶åå‘½ä»¤è¡Œ:
```
python setup.py install
```

éšååœ¨ä»£ç ä¸­ï¼š

```python3
from harvesttext import HarvestText
ht = HarvestText()
```

å³å¯è°ƒç”¨æœ¬åº“çš„åŠŸèƒ½æ¥å£ã€‚

<a id="å®ä½“é“¾æ¥"> </a>
### å®ä½“é“¾æ¥
ç»™å®šæŸäº›å®ä½“åŠå…¶å¯èƒ½çš„ä»£ç§°ï¼Œä»¥åŠå®ä½“å¯¹åº”ç±»å‹ã€‚å°†å…¶ç™»å½•åˆ°è¯å…¸ä¸­ï¼Œåœ¨åˆ†è¯æ—¶ä¼˜å…ˆåˆ‡åˆ†å‡ºæ¥ï¼Œå¹¶ä¸”ä»¥å¯¹åº”ç±»å‹ä½œä¸ºè¯æ€§ã€‚ä¹Ÿå¯ä»¥å•ç‹¬è·å¾—è¯­æ–™ä¸­çš„æ‰€æœ‰å®ä½“åŠå…¶ä½ç½®ï¼š

```python3
para = "ä¸Šæ¸¯çš„æ­¦ç£Šå’Œæ’å¤§çš„éƒœæ—ï¼Œè°æ˜¯ä¸­å›½æœ€å¥½çš„å‰é”‹ï¼Ÿé‚£å½“ç„¶æ˜¯æ­¦ç£Šæ­¦çƒç‹äº†ï¼Œä»–æ˜¯å°„æ‰‹æ¦œç¬¬ä¸€ï¼ŒåŸæ¥æ˜¯å¼±ç‚¹çš„å•åˆ€ä¹Ÿæœ‰äº†è¿›æ­¥"
entity_mention_dict = {'æ­¦ç£Š':['æ­¦ç£Š','æ­¦çƒç‹'],'éƒœæ—':['éƒœæ—','éƒœé£æœº'],'å‰é”‹':['å‰é”‹'],'ä¸Šæµ·ä¸Šæ¸¯':['ä¸Šæ¸¯'],'å¹¿å·æ’å¤§':['æ’å¤§'],'å•åˆ€çƒ':['å•åˆ€']}
entity_type_dict = {'æ­¦ç£Š':'çƒå‘˜','éƒœæ—':'çƒå‘˜','å‰é”‹':'ä½ç½®','ä¸Šæµ·ä¸Šæ¸¯':'çƒé˜Ÿ','å¹¿å·æ’å¤§':'çƒé˜Ÿ','å•åˆ€çƒ':'æœ¯è¯­'}
ht.add_entities(entity_mention_dict,entity_type_dict)
print("\nSentence segmentation")
print(ht.seg(para,return_sent=True))    # return_sent=Falseæ—¶ï¼Œåˆ™è¿”å›è¯è¯­åˆ—è¡¨
```

> ä¸Šæ¸¯ çš„ æ­¦ç£Š å’Œ æ’å¤§ çš„ éƒœæ— ï¼Œ è° æ˜¯ ä¸­å›½ æœ€å¥½ çš„ å‰é”‹ ï¼Ÿ é‚£ å½“ç„¶ æ˜¯ æ­¦ç£Š æ­¦çƒç‹ äº†ï¼Œ ä»– æ˜¯ å°„æ‰‹æ¦œ ç¬¬ä¸€ ï¼Œ åŸæ¥ æ˜¯ å¼±ç‚¹ çš„ å•åˆ€ ä¹Ÿ æœ‰ äº† è¿›æ­¥

é‡‡ç”¨ä¼ ç»Ÿçš„åˆ†è¯å·¥å…·å¾ˆå®¹æ˜“æŠŠâ€œæ­¦çƒç‹â€æ‹†åˆ†ä¸ºâ€œæ­¦ çƒç‹â€

è¯æ€§æ ‡æ³¨ï¼ŒåŒ…æ‹¬æŒ‡å®šçš„ç‰¹æ®Šç±»å‹ã€‚
```python3
print("\nPOS tagging with entity types")
for word, flag in ht.posseg(para):
	print("%s:%s" % (word, flag),end = " ")
```

> ä¸Šæ¸¯:çƒé˜Ÿ çš„:uj æ­¦ç£Š:çƒå‘˜ å’Œ:c æ’å¤§:çƒé˜Ÿ çš„:uj éƒœæ—:çƒå‘˜ ï¼Œ:x è°:r æ˜¯:v ä¸­å›½:ns æœ€å¥½:a çš„:uj å‰é”‹:ä½ç½® ï¼Ÿ:x é‚£:r å½“ç„¶:d æ˜¯:v æ­¦ç£Š:çƒå‘˜ æ­¦çƒç‹:çƒå‘˜ äº†:ul ï¼Œ:x ä»–:r æ˜¯:v å°„æ‰‹æ¦œ:n ç¬¬ä¸€:m ï¼Œ:x åŸæ¥:d æ˜¯:v å¼±ç‚¹:n çš„:uj å•åˆ€:æœ¯è¯­ ä¹Ÿ:d æœ‰:v äº†:ul è¿›æ­¥:d 

```python3
for span, entity in ht.entity_linking(para):
	print(span, entity)
```

> [0, 2] ('ä¸Šæµ·ä¸Šæ¸¯', '#çƒé˜Ÿ#')
[3, 5] ('æ­¦ç£Š', '#çƒå‘˜#')
[6, 8] ('å¹¿å·æ’å¤§', '#çƒé˜Ÿ#')
[9, 11] ('éƒœæ—', '#çƒå‘˜#')
[19, 21] ('å‰é”‹', '#ä½ç½®#')
[26, 28] ('æ­¦ç£Š', '#çƒå‘˜#')
[28, 31] ('æ­¦ç£Š', '#çƒå‘˜#')
[47, 49] ('å•åˆ€çƒ', '#æœ¯è¯­#')

è¿™é‡ŒæŠŠâ€œæ­¦çƒç‹â€è½¬åŒ–ä¸ºäº†æ ‡å‡†æŒ‡ç§°â€œæ­¦ç£Šâ€ï¼Œå¯ä»¥ä¾¿äºæ ‡å‡†ç»Ÿä¸€çš„ç»Ÿè®¡å·¥ä½œã€‚

åˆ†å¥ï¼š
```python3
print(ht.cut_sentences(para))
```

> ['ä¸Šæ¸¯çš„æ­¦ç£Šå’Œæ’å¤§çš„éƒœæ—ï¼Œè°æ˜¯ä¸­å›½æœ€å¥½çš„å‰é”‹ï¼Ÿ', 'é‚£å½“ç„¶æ˜¯æ­¦ç£Šæ­¦çƒç‹äº†ï¼Œä»–æ˜¯å°„æ‰‹æ¦œç¬¬ä¸€ï¼ŒåŸæ¥æ˜¯å¼±ç‚¹çš„å•åˆ€ä¹Ÿæœ‰äº†è¿›æ­¥']

å¦‚æœæ‰‹å¤´æš‚æ—¶æ²¡æœ‰å¯ç”¨çš„è¯å…¸ï¼Œä¸å¦¨çœ‹çœ‹æœ¬åº“[å†…ç½®èµ„æº](#å†…ç½®èµ„æº)ä¸­çš„é¢†åŸŸè¯å…¸æ˜¯å¦é€‚åˆä½ çš„éœ€è¦ã€‚

\*ç°åœ¨æœ¬åº“èƒ½å¤Ÿä¹Ÿç”¨ä¸€äº›åŸºæœ¬ç­–ç•¥æ¥å¤„ç†å¤æ‚çš„å®ä½“æ¶ˆæ­§ä»»åŠ¡ï¼ˆæ¯”å¦‚ä¸€è¯å¤šä¹‰ã€"è€å¸ˆ"æ˜¯æŒ‡"Aè€å¸ˆ"è¿˜æ˜¯"Bè€å¸ˆ"ï¼Ÿã€‘ã€å€™é€‰è¯é‡å ã€xxå¸‚é•¿/æ±Ÿyyï¼Ÿã€xxå¸‚é•¿/æ±Ÿyyï¼Ÿã€‘ï¼‰ã€‚
å…·ä½“å¯è§[linking_strategy()](./examples/basics.py#linking_strategy)

<a id="å‘½åå®ä½“è¯†åˆ«"> </a>
### å‘½åå®ä½“è¯†åˆ«
æ‰¾åˆ°ä¸€å¥å¥å­ä¸­çš„äººåï¼Œåœ°åï¼Œæœºæ„åç­‰å‘½åå®ä½“ã€‚ä½¿ç”¨äº† [pyhanLP](https://github.com/hankcs/pyhanlp) çš„æ¥å£å®ç°ã€‚

```python
ht0 = HarvestText()
sent = "ä¸Šæµ·ä¸Šæ¸¯è¶³çƒé˜Ÿçš„æ­¦ç£Šæ˜¯ä¸­å›½æœ€å¥½çš„å‰é”‹ã€‚"
print(ht0.named_entity_recognition(sent))
```

```
{'ä¸Šæµ·ä¸Šæ¸¯è¶³çƒé˜Ÿ': 'æœºæ„å', 'æ­¦ç£Š': 'äººå', 'ä¸­å›½': 'åœ°å'}
```

<a id="ä¾å­˜å¥æ³•åˆ†æ"> </a>
### ä¾å­˜å¥æ³•åˆ†æ
åˆ†æè¯­å¥ä¸­å„ä¸ªè¯è¯­ï¼ˆåŒ…æ‹¬é“¾æ¥åˆ°çš„å®ä½“ï¼‰çš„ä¸»è°“å®¾è¯­ä¿®é¥°ç­‰è¯­æ³•å…³ç³»ï¼Œå¹¶ä»¥æ­¤æå–å¯èƒ½çš„äº‹ä»¶ä¸‰å…ƒç»„ã€‚ä½¿ç”¨äº† [pyhanLP](https://github.com/hankcs/pyhanlp) çš„æ¥å£å®ç°ã€‚

```python
ht0 = HarvestText()
para = "ä¸Šæ¸¯çš„æ­¦ç£Šæ­¦çƒç‹æ˜¯ä¸­å›½æœ€å¥½çš„å‰é”‹ã€‚"
entity_mention_dict = {'æ­¦ç£Š': ['æ­¦ç£Š', 'æ­¦çƒç‹'], "ä¸Šæµ·ä¸Šæ¸¯":["ä¸Šæ¸¯"]}
entity_type_dict = {'æ­¦ç£Š': 'çƒå‘˜', "ä¸Šæµ·ä¸Šæ¸¯":"çƒé˜Ÿ"}
ht0.add_entities(entity_mention_dict, entity_type_dict)
for arc in ht0.dependency_parse(para):
    print(arc)
print(ht0.triple_extraction(para))
```

```
[0, 'ä¸Šæ¸¯', 'çƒé˜Ÿ', 'å®šä¸­å…³ç³»', 3]
[1, 'çš„', 'u', 'å³é™„åŠ å…³ç³»', 0]
[2, 'æ­¦ç£Š', 'çƒå‘˜', 'å®šä¸­å…³ç³»', 3]
[3, 'æ­¦çƒç‹', 'çƒå‘˜', 'ä¸»è°“å…³ç³»', 4]
[4, 'æ˜¯', 'v', 'æ ¸å¿ƒå…³ç³»', -1]
[5, 'ä¸­å›½', 'ns', 'å®šä¸­å…³ç³»', 8]
[6, 'æœ€å¥½', 'd', 'å®šä¸­å…³ç³»', 8]
[7, 'çš„', 'u', 'å³é™„åŠ å…³ç³»', 6]
[8, 'å‰é”‹', 'n', 'åŠ¨å®¾å…³ç³»', 4]
[9, 'ã€‚', 'w', 'æ ‡ç‚¹ç¬¦å·', 4]
```
```python
print(ht0.triple_extraction(para))
```
```
[['ä¸Šæ¸¯æ­¦ç£Šæ­¦çƒç‹', 'æ˜¯', 'ä¸­å›½æœ€å¥½å‰é”‹']]
```

<a id="å­—ç¬¦æ‹¼éŸ³çº é”™"> </a>

### å­—ç¬¦æ‹¼éŸ³çº é”™
æŠŠè¯­å¥ä¸­æœ‰å¯èƒ½æ˜¯å·²çŸ¥å®ä½“çš„é”™è¯¯æ‹¼å†™ï¼ˆè¯¯å·®ä¸€ä¸ªå­—ç¬¦æˆ–æ‹¼éŸ³ï¼‰çš„è¯è¯­é“¾æ¥åˆ°å¯¹åº”å®ä½“ã€‚
```python
def entity_error_check():
    ht0 = HarvestText()
    typed_words = {"äººå":["æ­¦ç£Š"]}
    ht0.add_typed_words(typed_words)
    sent1 = "æ­¦ç£Šå’Œå´åŠ›åªå·®ä¸€ä¸ªæ‹¼éŸ³"
    print(sent1)
    print(ht0.entity_linking(sent1, pinyin_recheck=True))
    sent2 = "æ­¦ç£Šå’Œå´ç£Šåªå·®ä¸€ä¸ªå­—"
    print(sent2)
    print(ht0.entity_linking(sent2, char_recheck=True))
    sent3 = "å´ç£Šå’Œå´åŠ›éƒ½å¯èƒ½æ˜¯æ­¦ç£Šçš„ä»£ç§°"
    print(sent3)
    print(ht0.get_linking_mention_candidates(sent3, pinyin_recheck=True, char_recheck=True))
entity_error_check()
```

```
æ­¦ç£Šå’Œå´åŠ›åªå·®ä¸€ä¸ªæ‹¼éŸ³
[([0, 2], ('æ­¦ç£Š', '#äººå#')), [(3, 5), ('æ­¦ç£Š', '#äººå#')]]
æ­¦ç£Šå’Œå´ç£Šåªå·®ä¸€ä¸ªå­—
[([0, 2], ('æ­¦ç£Š', '#äººå#')), [(3, 5), ('æ­¦ç£Š', '#äººå#')]]
å´ç£Šå’Œå´åŠ›éƒ½å¯èƒ½æ˜¯æ­¦ç£Šçš„ä»£ç§°
('å´ç£Šå’Œå´åŠ›éƒ½å¯èƒ½æ˜¯æ­¦ç£Šçš„ä»£ç§°', defaultdict(<class 'list'>, {(0, 2): {'æ­¦ç£Š'}, (3, 5): {'æ­¦ç£Š'}}))
```
<a id="æƒ…æ„Ÿåˆ†æ"> </a>

### æƒ…æ„Ÿåˆ†æ
æœ¬åº“é‡‡ç”¨æƒ…æ„Ÿè¯å…¸æ–¹æ³•è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œé€šè¿‡æä¾›å°‘é‡æ ‡å‡†çš„è¤’è´¬ä¹‰è¯è¯­ï¼ˆâ€œç§å­è¯â€ï¼‰ï¼Œä»è¯­æ–™ä¸­è‡ªåŠ¨å­¦ä¹ å…¶ä»–è¯è¯­çš„æƒ…æ„Ÿå€¾å‘ï¼Œå½¢æˆæƒ…æ„Ÿè¯å…¸ã€‚å¯¹å¥ä¸­æƒ…æ„Ÿè¯çš„åŠ æ€»å¹³å‡åˆ™ç”¨äºåˆ¤æ–­å¥å­çš„æƒ…æ„Ÿå€¾å‘ï¼š

```python3
print("\nsentiment dictionary")
sents = ["æ­¦ç£Šå¨æ­¦ï¼Œä¸­è¶…ç¬¬ä¸€å°„æ‰‹ï¼",
      "æ­¦ç£Šå¼ºï¼Œä¸­è¶…æœ€ç¬¬ä¸€æœ¬åœŸçƒå‘˜ï¼",
      "éƒœæ—ä¸è¡Œï¼Œåªä¼šæŠ±æ€¨çš„çƒå‘˜æ³¨å®šä¸Šé™äº†",
      "éƒœæ—çœ‹æ¥ä¸è¡Œï¼Œå·²ç»åˆ°ä¸Šé™äº†"]
sent_dict = ht.build_sent_dict(sents,min_times=1,pos_seeds=["ç¬¬ä¸€"],neg_seeds=["ä¸è¡Œ"])
print("%s:%f" % ("å¨æ­¦",sent_dict["å¨æ­¦"]))
print("%s:%f" % ("çƒå‘˜",sent_dict["çƒå‘˜"]))
print("%s:%f" % ("ä¸Šé™",sent_dict["ä¸Šé™"]))
```

> sentiment dictionary 
> å¨æ­¦:1.000000 
> çƒå‘˜:0.000000 
> ä¸Šé™:-1.000000

```python3
print("\nsentence sentiment")
sent = "æ­¦çƒç‹å¨æ­¦ï¼Œä¸­è¶…æœ€å¼ºçƒå‘˜ï¼"
print("%f:%s" % (ht.analyse_sent(sent),sent))
```
> 0.600000:æ­¦çƒç‹å¨æ­¦ï¼Œä¸­è¶…æœ€å¼ºçƒå‘˜ï¼

å¦‚æœæ²¡æƒ³å¥½é€‰æ‹©å“ªäº›è¯è¯­ä½œä¸ºâ€œç§å­è¯â€ï¼Œæœ¬åº“ä¸­ä¹Ÿå†…ç½®äº†ä¸€ä¸ªé€šç”¨æƒ…æ„Ÿè¯å…¸[å†…ç½®èµ„æº](#å†…ç½®èµ„æº)ï¼Œå¯ä»¥ä»ä¸­æŒ‘é€‰ã€‚

<a id="ä¿¡æ¯æ£€ç´¢"> </a>

### ä¿¡æ¯æ£€ç´¢
å¯ä»¥ä»æ–‡æ¡£åˆ—è¡¨ä¸­æŸ¥æ‰¾å‡ºåŒ…å«å¯¹åº”å®ä½“ï¼ˆåŠå…¶åˆ«ç§°ï¼‰çš„æ–‡æ¡£ï¼Œä»¥åŠç»Ÿè®¡åŒ…å«æŸå®ä½“çš„æ–‡æ¡£æ•°ã€‚ä½¿ç”¨å€’æ’ç´¢å¼•çš„æ•°æ®ç»“æ„å®Œæˆå¿«é€Ÿæ£€ç´¢ã€‚
```python3
docs = ["æ­¦ç£Šå¨æ­¦ï¼Œä¸­è¶…ç¬¬ä¸€å°„æ‰‹ï¼",
		"éƒœæ—çœ‹æ¥ä¸è¡Œï¼Œå·²ç»åˆ°ä¸Šé™äº†ã€‚",
		"æ­¦çƒç‹å¨æ­¦ï¼Œä¸­è¶…æœ€å¼ºå‰é”‹ï¼",
		"æ­¦ç£Šå’Œéƒœæ—ï¼Œè°æ˜¯ä¸­å›½æœ€å¥½çš„å‰é”‹ï¼Ÿ"]
inv_index = ht.build_index(docs)
print(ht.get_entity_counts(docs, inv_index))  # è·å¾—æ–‡æ¡£ä¸­æ‰€æœ‰å®ä½“çš„å‡ºç°æ¬¡æ•°
# {'æ­¦ç£Š': 3, 'éƒœæ—': 2, 'å‰é”‹': 2}

print(ht.search_entity("æ­¦ç£Š", docs, inv_index))  # å•å®ä½“æŸ¥æ‰¾
# ['æ­¦ç£Šå¨æ­¦ï¼Œä¸­è¶…ç¬¬ä¸€å°„æ‰‹ï¼', 'æ­¦çƒç‹å¨æ­¦ï¼Œä¸­è¶…æœ€å¼ºå‰é”‹ï¼', 'æ­¦ç£Šå’Œéƒœæ—ï¼Œè°æ˜¯ä¸­å›½æœ€å¥½çš„å‰é”‹ï¼Ÿ']

print(ht.search_entity("æ­¦ç£Š éƒœæ—", docs, inv_index))  # å¤šå®ä½“å…±ç°
# ['æ­¦ç£Šå’Œéƒœæ—ï¼Œè°æ˜¯ä¸­å›½æœ€å¥½çš„å‰é”‹ï¼Ÿ']

# è°æ˜¯æœ€è¢«äººä»¬çƒ­è®®çš„å‰é”‹ï¼Ÿç”¨è¿™é‡Œçš„æ¥å£å¯ä»¥å¾ˆç®€ä¾¿åœ°å›ç­”è¿™ä¸ªé—®é¢˜
subdocs = ht.search_entity("#çƒå‘˜# å‰é”‹", docs, inv_index)
print(subdocs)  # å®ä½“ã€å®ä½“ç±»å‹æ··åˆæŸ¥æ‰¾
# ['æ­¦çƒç‹å¨æ­¦ï¼Œä¸­è¶…æœ€å¼ºå‰é”‹ï¼', 'æ­¦ç£Šå’Œéƒœæ—ï¼Œè°æ˜¯ä¸­å›½æœ€å¥½çš„å‰é”‹ï¼Ÿ']
inv_index2 = ht.build_index(subdocs)
print(ht.get_entity_counts(subdocs, inv_index2, used_type=["çƒå‘˜"]))  # å¯ä»¥é™å®šç±»å‹
# {'æ­¦ç£Š': 2, 'éƒœæ—': 1}
```

<a id="å…³ç³»ç½‘ç»œ"> </a>
### å…³ç³»ç½‘ç»œ
(ä½¿ç”¨networkxå®ç°)
åˆ©ç”¨è¯å…±ç°å…³ç³»ï¼Œå»ºç«‹å…¶å®ä½“é—´å›¾ç»“æ„çš„ç½‘ç»œå…³ç³»(è¿”å›networkx.Graphç±»å‹)ã€‚å¯ä»¥ç”¨æ¥å»ºç«‹äººç‰©ä¹‹é—´çš„ç¤¾äº¤ç½‘ç»œç­‰ã€‚
```python3
# åœ¨ç°æœ‰å®ä½“åº“çš„åŸºç¡€ä¸Šéšæ—¶æ–°å¢ï¼Œæ¯”å¦‚ä»æ–°è¯å‘ç°ä¸­å¾—åˆ°çš„æ¼ç½‘ä¹‹é±¼
ht.add_new_entity("é¢œéªå‡Œ", "é¢œéªå‡Œ", "çƒå‘˜")
docs = ["æ­¦ç£Šå’Œé¢œéªå‡Œæ˜¯é˜Ÿå‹",
		"æ­¦ç£Šå’Œéƒœæ—éƒ½æ˜¯å›½å†…é¡¶å°–å‰é”‹"]
G = ht.build_entity_graph(docs)
print(dict(G.edges.items()))
G = ht.build_entity_graph(docs, used_types=["çƒå‘˜"])
print(dict(G.edges.items()))
```

è·å¾—ä»¥ä¸€ä¸ªè¯è¯­ä¸ºä¸­å¿ƒçš„è¯è¯­ç½‘ç»œï¼Œä¸‹é¢ä»¥ä¸‰å›½ç¬¬ä¸€ç« ä¸ºä¾‹ï¼Œæ¢ç´¢ä¸»äººå…¬åˆ˜å¤‡çš„é­é‡ï¼ˆä¸‹ä¸ºä¸»è¦ä»£ç ï¼Œä¾‹å­è§[build_word_ego_graph()](./examples/basics.py#linking_strategy)ï¼‰ã€‚
```python3
entity_mention_dict, entity_type_dict = get_sanguo_entity_dict()
ht0.add_entities(entity_mention_dict, entity_type_dict)
sanguo1 = get_sanguo()[0]
stopwords = get_baidu_stopwords()
docs = ht0.cut_sentences(sanguo1)
G = ht0.build_word_ego_graph(docs,"åˆ˜å¤‡",min_freq=3,other_min_freq=2,stopwords=stopwords)
```
![word_ego_net](/images/word_ego_net.jpg)

åˆ˜å…³å¼ ä¹‹æƒ…è°Šï¼Œåˆ˜å¤‡æŠ•å¥”çš„é å±±ï¼Œä»¥åŠåˆ˜å¤‡è®¨è´¼ä¹‹ç»å†å°½åœ¨äºæ­¤ã€‚

<a id="æ–‡æœ¬æ‘˜è¦"> </a>
### æ–‡æœ¬æ‘˜è¦
(ä½¿ç”¨networkxå®ç°)
ä½¿ç”¨Textrankç®—æ³•ï¼Œå¾—åˆ°ä»æ–‡æ¡£é›†åˆä¸­æŠ½å–ä»£è¡¨å¥ä½œä¸ºæ‘˜è¦ä¿¡æ¯ï¼š
```python3
print("\nText summarization")
docs = ["æ­¦ç£Šå¨æ­¦ï¼Œä¸­è¶…ç¬¬ä¸€å°„æ‰‹ï¼",
		"éƒœæ—çœ‹æ¥ä¸è¡Œï¼Œå·²ç»åˆ°ä¸Šé™äº†ã€‚",
		"æ­¦çƒç‹å¨æ­¦ï¼Œä¸­è¶…æœ€å¼ºå‰é”‹ï¼",
		"æ­¦ç£Šå’Œéƒœæ—ï¼Œè°æ˜¯ä¸­å›½æœ€å¥½çš„å‰é”‹ï¼Ÿ"]
for doc in ht.get_summary(docs, topK=2):
	print(doc)
# æ­¦çƒç‹å¨æ­¦ï¼Œä¸­è¶…æœ€å¼ºå‰é”‹ï¼
# æ­¦ç£Šå¨æ­¦ï¼Œä¸­è¶…ç¬¬ä¸€å°„æ‰‹ï¼	
```


<a id="å†…ç½®èµ„æº"> </a>
### å†…ç½®èµ„æº
ç°åœ¨æœ¬åº“å†…é›†æˆäº†ä¸€äº›èµ„æºï¼Œæ–¹ä¾¿ä½¿ç”¨å’Œå»ºç«‹demoã€‚

èµ„æºåŒ…æ‹¬ï¼š
- è¤’è´¬ä¹‰è¯å…¸ æ¸…åå¤§å­¦ æå†› æ•´ç†è‡ªhttp://nlp.csai.tsinghua.edu.cn/site2/index.php/13-sms
- ç™¾åº¦åœç”¨è¯è¯å…¸ æ¥è‡ªç½‘ç»œï¼šhttps://wenku.baidu.com/view/98c46383e53a580216fcfed9.html
- é¢†åŸŸè¯å…¸ æ¥è‡ªæ¸…åTHUNLPï¼š http://thuocl.thunlp.org/ å…¨éƒ¨ç±»å‹`['IT', 'åŠ¨ç‰©', 'åŒ»è¯', 'å†å²äººå', 'åœ°å', 'æˆè¯­', 'æ³•å¾‹', 'è´¢ç»', 'é£Ÿç‰©']`


æ­¤å¤–ï¼Œè¿˜æä¾›äº†ä¸€ä¸ªç‰¹æ®Šèµ„æºâ€”â€”ã€Šä¸‰å›½æ¼”ä¹‰ã€‹ï¼ŒåŒ…æ‹¬ï¼š
- ä¸‰å›½æ¼”ä¹‰æ–‡è¨€æ–‡æ–‡æœ¬
- ä¸‰å›½æ¼”ä¹‰äººåã€å·åã€åŠ¿åŠ›çŸ¥è¯†åº“

å¤§å®¶å¯ä»¥æ¢ç´¢ä»å…¶ä¸­èƒ½å¤Ÿå¾—åˆ°ä»€ä¹ˆæœ‰è¶£å‘ç°ğŸ˜ã€‚

```python3
def load_resources():
	from harvesttext.resources import get_qh_sent_dict,get_baidu_stopwords,get_sanguo,get_sanguo_entity_dict
    sdict = get_qh_sent_dict()              # {"pos":[ç§¯æè¯...],"neg":[æ¶ˆæè¯...]}
    print("pos_words:",list(sdict["pos"])[10:15])
    print("neg_words:",list(sdict["neg"])[5:10])

    stopwords = get_baidu_stopwords()
    print("stopwords:", list(stopwords)[5:10])

    docs = get_sanguo()                 # æ–‡æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ç« çš„æ–‡æœ¬
    print("ä¸‰å›½æ¼”ä¹‰æœ€åä¸€ç« æœ«16å­—:\n",docs[-1][-16:])
    entity_mention_dict, entity_type_dict = get_sanguo_entity_dict()
    print("åˆ˜å¤‡ æŒ‡ç§°ï¼š",entity_mention_dict["åˆ˜å¤‡"])
    print("åˆ˜å¤‡ ç±»åˆ«ï¼š",entity_type_dict["åˆ˜å¤‡"])
    print("èœ€ ç±»åˆ«ï¼š", entity_type_dict["èœ€"])
    print("ç›Šå· ç±»åˆ«ï¼š", entity_type_dict["ç›Šå·"])
load_resources()
```

```
pos_words: ['å®°ç›¸è‚šé‡Œå¥½æ’‘èˆ¹', 'æŸ¥å®', 'å¿ å®', 'åæ‰‹', 'èªæ˜']
neg_words: ['æ•£æ¼«', 'è°—è¨€', 'è¿‚æ‰§', 'è‚ è‚¥è„‘æ»¡', 'å‡ºå–']
stopwords: ['apart', 'å·¦å³', 'ç»“æœ', 'probably', 'think']
ä¸‰å›½æ¼”ä¹‰æœ€åä¸€ç« æœ«16å­—:
 é¼è¶³ä¸‰åˆ†å·²æˆæ¢¦ï¼Œåäººå‡­åŠç©ºç‰¢éªšã€‚
åˆ˜å¤‡ æŒ‡ç§°ï¼š ['åˆ˜å¤‡', 'åˆ˜ç„å¾·', 'ç„å¾·']
åˆ˜å¤‡ ç±»åˆ«ï¼š äººå
èœ€ ç±»åˆ«ï¼š åŠ¿åŠ›
ç›Šå· ç±»åˆ«ï¼š å·å
```

åŠ è½½æ¸…åé¢†åŸŸè¯å…¸ï¼Œå¹¶ä½¿ç”¨åœç”¨è¯ã€‚
```python3
def using_typed_words():
    from harvesttext.resources import get_qh_typed_words,get_baidu_stopwords
    ht0 = HarvestText()
    typed_words, stopwords = get_qh_typed_words(), get_baidu_stopwords()
    ht0.add_typed_words(typed_words)
    sentence = "THUOCLæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„ä¸€å¥—ä¸­æ–‡è¯åº“ï¼Œè¯è¡¨æ¥è‡ªä¸»æµç½‘ç«™çš„ç¤¾ä¼šæ ‡ç­¾ã€æœç´¢çƒ­è¯ã€è¾“å…¥æ³•è¯åº“ç­‰ã€‚"
    print(sentence)
    print(ht0.posseg(sentence,stopwords=stopwords))
using_typed_words()
```

```
THUOCLæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„ä¸€å¥—ä¸­æ–‡è¯åº“ï¼Œè¯è¡¨æ¥è‡ªä¸»æµç½‘ç«™çš„ç¤¾ä¼šæ ‡ç­¾ã€æœç´¢çƒ­è¯ã€è¾“å…¥æ³•è¯åº“ç­‰ã€‚
[('THUOCL', 'eng'), ('è‡ªç„¶è¯­è¨€å¤„ç†', 'IT'), ('ä¸€å¥—', 'm'), ('ä¸­æ–‡', 'nz'), ('è¯åº“', 'n'), ('è¯è¡¨', 'n'), ('æ¥è‡ª', 'v'), ('ä¸»æµ', 'b'), ('ç½‘ç«™', 'n'), ('ç¤¾ä¼š', 'n'), ('æ ‡ç­¾', 'è´¢ç»'), ('æœç´¢', 'v'), ('çƒ­è¯', 'n'), ('è¾“å…¥æ³•', 'IT'), ('è¯åº“', 'n')]
```

ä¸€äº›è¯è¯­è¢«èµ‹äºˆç‰¹æ®Šç±»å‹IT,è€Œâ€œæ˜¯â€ç­‰è¯è¯­è¢«ç­›å‡ºã€‚


<a id="æ–°è¯å‘ç°"> </a>
### æ–°è¯å‘ç°
ä»æ¯”è¾ƒå¤§é‡çš„æ–‡æœ¬ä¸­åˆ©ç”¨ä¸€äº›ç»Ÿè®¡æŒ‡æ ‡å‘ç°æ–°è¯ã€‚ï¼ˆå¯é€‰ï¼‰é€šè¿‡æä¾›ä¸€äº›ç§å­è¯è¯­æ¥ç¡®å®šæ€æ ·ç¨‹åº¦è´¨é‡çš„è¯è¯­å¯ä»¥è¢«å‘ç°ã€‚ï¼ˆå³è‡³å°‘æ‰€æœ‰çš„ç§å­è¯ä¼šè¢«å‘ç°ï¼Œåœ¨æ»¡è¶³ä¸€å®šçš„åŸºç¡€è¦æ±‚çš„å‰æä¸‹ã€‚ï¼‰
```python3
para = "ä¸Šæ¸¯çš„æ­¦ç£Šå’Œæ’å¤§çš„éƒœæ—ï¼Œè°æ˜¯ä¸­å›½æœ€å¥½çš„å‰é”‹ï¼Ÿé‚£å½“ç„¶æ˜¯æ­¦ç£Šæ­¦çƒç‹äº†ï¼Œä»–æ˜¯å°„æ‰‹æ¦œç¬¬ä¸€ï¼ŒåŸæ¥æ˜¯å¼±ç‚¹çš„å•åˆ€ä¹Ÿæœ‰äº†è¿›æ­¥"
#è¿”å›å…³äºæ–°è¯è´¨é‡çš„ä¸€ç³»åˆ—ä¿¡æ¯ï¼Œå…è®¸æ‰‹å·¥æ”¹è¿›ç­›é€‰(pd.DataFrameå‹)
new_words_info = ht.word_discover(para)
#new_words_info = ht.word_discover(para, threshold_seeds=["æ­¦ç£Š"])  
new_words = new_words_info.index.tolist()
print(new_words)
```

> ["æ­¦ç£Š"]

å…·ä½“çš„æ–¹æ³•å’ŒæŒ‡æ ‡å«ä¹‰ï¼Œå‚è€ƒï¼šhttp://www.matrix67.com/blog/archives/5044

å‘ç°çš„æ–°è¯å¾ˆå¤šéƒ½å¯èƒ½æ˜¯æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå…³é”®è¯ï¼Œæ•…å¯ä»¥æŠŠæ‰¾åˆ°çš„æ–°è¯ç™»å½•ï¼Œä½¿åç»­çš„åˆ†è¯ä¼˜å…ˆåˆ†å‡ºè¿™äº›è¯ã€‚
```python3
def new_word_register():
    new_words = ["è½å¶çƒ","666"]
    ht.add_new_words(new_words)   # ä½œä¸ºå¹¿ä¹‰ä¸Šçš„"æ–°è¯"ç™»å½•
    ht.add_new_entity("è½å¶çƒ", mention0="è½å¶çƒ", type0="æœ¯è¯­")  # ä½œä¸ºç‰¹å®šç±»å‹ç™»å½•
    print(ht.seg("è¿™ä¸ªè½å¶çƒè¸¢å¾—çœŸæ˜¯666", return_sent=True))
    for word, flag in ht.posseg("è¿™ä¸ªè½å¶çƒè¸¢å¾—çœŸæ˜¯666"):
        print("%s:%s" % (word, flag), end=" ")
```
> è¿™ä¸ª è½å¶çƒ è¸¢ å¾— çœŸæ˜¯ 666

> è¿™ä¸ª:r è½å¶çƒ:æœ¯è¯­ è¸¢:v å¾—:ud çœŸæ˜¯:d 666:æ–°è¯ 

ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸€äº›ç‰¹æ®Šçš„*è§„åˆ™*æ¥æ‰¾åˆ°æ‰€éœ€çš„å…³é”®è¯ï¼Œå¹¶ç›´æ¥èµ‹äºˆç±»å‹ï¼Œæ¯”å¦‚å…¨è‹±æ–‡ï¼Œæˆ–è€…æœ‰ç€ç‰¹å®šçš„å‰åç¼€ç­‰ã€‚
```python3
# find_with_rules()
from harvesttext.match_patterns import UpperFirst, AllEnglish, Contains, StartsWith, EndsWith
text0 = "æˆ‘å–œæ¬¢Pythonï¼Œå› ä¸ºrequestsåº“å¾ˆé€‚åˆçˆ¬è™«"
ht0 = HarvestText()

found_entities = ht0.find_entity_with_rule(text0, rulesets=[AllEnglish()], type0="è‹±æ–‡å")
print(found_entities)
print(ht0.posseg(text0))
```

```
{'Python', 'requests'}
[('æˆ‘', 'r'), ('å–œæ¬¢', 'v'), ('Python', 'è‹±æ–‡å'), ('ï¼Œ', 'x'), ('å› ä¸º', 'c'), ('requests', 'è‹±æ–‡å'), ('åº“', 'n'), ('å¾ˆ', 'd'), ('é€‚åˆ', 'v'), ('çˆ¬è™«', 'n')]
```


<a id="å­˜å–ä¸æ¶ˆé™¤"> </a>
### å­˜å–æ¶ˆé™¤
å¯ä»¥æœ¬åœ°ä¿å­˜æ¨¡å‹å†è¯»å–å¤ç”¨ï¼Œä¹Ÿå¯ä»¥æ¶ˆé™¤å½“å‰æ¨¡å‹çš„è®°å½•ã€‚

```python3
from harvesttext import loadHT,saveHT
para = "ä¸Šæ¸¯çš„æ­¦ç£Šå’Œæ’å¤§çš„éƒœæ—ï¼Œè°æ˜¯ä¸­å›½æœ€å¥½çš„å‰é”‹ï¼Ÿé‚£å½“ç„¶æ˜¯æ­¦ç£Šæ­¦çƒç‹äº†ï¼Œä»–æ˜¯å°„æ‰‹æ¦œç¬¬ä¸€ï¼ŒåŸæ¥æ˜¯å¼±ç‚¹çš„å•åˆ€ä¹Ÿæœ‰äº†è¿›æ­¥"
saveHT(ht,"ht_model1")
ht2 = loadHT("ht_model1")

# æ¶ˆé™¤è®°å½•
ht2.clear()
print("cut with cleared model")
print(ht2.seg(para))
```

<a id="ç®€æ˜“é—®ç­”ç³»ç»Ÿ"> </a>
### ç®€æ˜“é—®ç­”ç³»ç»Ÿ
å…·ä½“å®ç°åŠä¾‹å­åœ¨[naiveKGQA.py](./examples/naiveKGQA.py)ä¸­ï¼Œä¸‹é¢ç»™å‡ºéƒ¨åˆ†ç¤ºæ„ï¼š

```python
QA = NaiveKGQA(SVOs, entity_type_dict=entity_type_dict)
questions = ["ä½ å¥½","å­™ä¸­å±±å¹²äº†ä»€ä¹ˆäº‹ï¼Ÿ","è°å‘åŠ¨äº†ä»€ä¹ˆï¼Ÿ","æ¸…æ”¿åºœç­¾è®¢äº†å“ªäº›æ¡çº¦ï¼Ÿ",
			 "è‹±å›½ä¸é¸¦ç‰‡æˆ˜äº‰çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ","è°å¤è¾Ÿäº†å¸åˆ¶ï¼Ÿ"]
for question0 in questions:
	print("é—®ï¼š"+question0)
	print("ç­”ï¼š"+QA.answer(question0))
```

```
é—®ï¼šå­™ä¸­å±±å¹²äº†ä»€ä¹ˆäº‹ï¼Ÿ
ç­”ï¼šå°±ä»»ä¸´æ—¶å¤§æ€»ç»Ÿã€å‘åŠ¨æŠ¤æ³•è¿åŠ¨ã€è®©ä½äºè¢ä¸–å‡¯
é—®ï¼šè°å‘åŠ¨äº†ä»€ä¹ˆï¼Ÿ
ç­”ï¼šè‹±æ³•è”å†›ä¾µç•¥ä¸­å›½ã€å›½æ°‘å…šäººäºŒæ¬¡é©å‘½ã€è‹±å›½é¸¦ç‰‡æˆ˜äº‰ã€æ—¥æœ¬ä¾µç•¥æœé²œã€å­™ä¸­å±±æŠ¤æ³•è¿åŠ¨ã€æ³•å›½ä¾µç•¥è¶Šå—ã€è‹±å›½ä¾µç•¥ä¸­å›½è¥¿è—æˆ˜äº‰ã€æ…ˆç¦§å¤ªåæˆŠæˆŒæ”¿å˜
é—®ï¼šæ¸…æ”¿åºœç­¾è®¢äº†å“ªäº›æ¡çº¦ï¼Ÿ
ç­”ï¼šåŒ—äº¬æ¡çº¦ã€å¤©æ´¥æ¡çº¦
é—®ï¼šè‹±å›½ä¸é¸¦ç‰‡æˆ˜äº‰çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ
ç­”ï¼šå‘åŠ¨
é—®ï¼šè°å¤è¾Ÿäº†å¸åˆ¶ï¼Ÿ
ç­”ï¼šè¢ä¸–å‡¯
```

## More
æœ¬åº“æ­£åœ¨å¼€å‘ä¸­ï¼Œå…³äºç°æœ‰åŠŸèƒ½çš„æ”¹å–„å’Œæ›´å¤šåŠŸèƒ½çš„æ·»åŠ å¯èƒ½ä¼šé™†ç»­åˆ°æ¥ã€‚æ¬¢è¿åœ¨issuesé‡Œæä¾›æ„è§å»ºè®®ã€‚è§‰å¾—å¥½ç”¨çš„è¯ï¼Œä¹Ÿä¸å¦¨æ¥ä¸ªStar~

æ„Ÿè°¢ä»¥ä¸‹repoå¸¦æ¥çš„å¯å‘ï¼š

[snownlp](https://github.com/isnowfy/snownlp)

[pyhanLP](https://github.com/hankcs/pyhanlp)

[funNLP](https://github.com/fighting41love/funNLP)

[ChineseWordSegmentation](https://github.com/Moonshile/ChineseWordSegmentation)

[EventTriplesExtraction](https://github.com/liuhuanyong/EventTriplesExtraction)



