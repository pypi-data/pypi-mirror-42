# ODESSA/AMI plugin for pyannote.database

## Installation

```bash
$ pip install pyannote.db.odessa.ami
```

You should then download the dataset files. This repo provides a download script for the required files in the `AMI/db_download/` folder. You can download that file independently from the repository, and run it:

```bash
$ bash ./download.sh /where/you/want/to/download/the/data/
```

You can also download them "by hand" on the [official website](http://groups.inf.ed.ac.uk/ami/download/) by checking all the AMI meetings and only the Headset mix stream.

For some reason, `scipy.io.wavfile` (heavily used in `pyannote.audio`) cannot read some of those files (`ValueError: Incomplete wav chunk.`). See https://github.com/pyannote/pyannote-audio/issues/146#issuecomment-461733210 for a solution to this problem.

Then, tell `pyannote` where to look for AMI audio files.

```bash
$ cat ~/.pyannote/db.yml
AMI: /path/to/amicorpus/*/audio/{uri}.wav
```

## Speaker diarization protocol

Protocol is initialized as follows:

```python
>>> from pyannote.database import get_protocol, FileFinder
>>> preprocessors = {'audio': FileFinder()}
>>> protocol = get_protocol('AMI.SpeakerDiarization.MixHeadset',
...                         preprocessors=preprocessors)
```

### Training

For background training (e.g.
[GMM/UBM](http://www.sciencedirect.com/science/article/pii/S1051200499903615), [i-vector](http://ieeexplore.ieee.org/document/5545402/), or [TristouNet neural embedding](https://arxiv.org/abs/1609.04301)), files of the training set can be
iterated using `protocol.train()`:

```python
>>> for current_file in protocol.train():
...
...     # path to the audio file
...     audio = current_file['audio']
...
...     # "who speaks when" reference
...     reference = current_file['annotation']
...
...     # when current_file has an 'annotated' key, it indicates that
...     # annotations are only provided for some regions of the file.
...     annotated = current_file['annotated']
...     # See http://pyannote.github.io/pyannote-core/structure.html#timeline
...
...     # train...
```

`reference` is a pyannote.core.Annotation. In particular, it can be iterated
as follows:

```python
>>> for segment, _, speaker in reference.itertracks(yield_label=True):
...
...     print('Speaker {speaker} speaks between {start}s and {end}s'.format(
...         speaker=speaker, start=segment.start, end=segment.end))
```
See http://pyannote.github.io/pyannote-core/structure.html#annotation for
details on other ways to use this data structure.

### Development

`protocol.development()` is the same as `protocol.train()` except it iterates
over the development set (e.g. for hyper-parameter tuning).

### Test / Evaluation

```python
>>> # initialize evaluation metric
>>> from pyannote.metrics.diarization import DiarizationErrorRate
>>> metric = DiarizationErrorRate()
>>>
>>> # iterate over each file of the test set
>>> for test_file in protocol.test():
...
...     # process test file
...     audio = test_file['audio']
...     hypothesis = process_file(audio)
...
...     # evaluate hypothesis
...     reference = test_file['annotation']
...     uem = test_file['annotated']
...     metric(reference, hypothesis, uem=uem)
>>>
>>> # report results
>>> metric.report(display=True)
```

## Speaker spotting procotol

In order to use the AMI dataset for the evaluation of speaker spotting systems,
the original speaker diarization training/development/test split has been
redefined.

Moreover, original files have also been split into shorter sessions in order to
increase the number of trials.

More details can be found in [this paper](https://www.isca-speech.org/archive/Odyssey_2018/pdfs/60.pdf): 

```bibtex
@inproceedings{Patino2018,
  Title = {{Low-Latency Speaker Spotting with Online Diarization and Detection}},
  Author = {Jose Patino and Ruiqing Yin and H\'{e}ctor Delgado and Herv\'{e} Bredin and Alain Komaty and Guillaume Wisniewski and Claude Barras and Nicholas Evans and S\'{e}bastien Marcel},
  Booktitle = {{Odyssey 2018, The Speaker and Language Recognition Workshop}},
  Pages = {140--146},
  Year = {2018},
  Month = {June},
  Address = {Les Sables d'Olonnes, France},
  url = {http://dx.doi.org/10.21437/Odyssey.2018-20},
}
```

Protocol is initialized as follows:

```python
>>> from pyannote.database import get_protocol, FileFinder
>>> preprocessors = {'audio': FileFinder()}
>>> protocol = get_protocol('AMI.SpeakerSpotting.MixHeadset',
...                         preprocessors=preprocessors)
```


### Training

`protocol.train()` can be used like in the speaker diarization protocol above.

### Enrolment

```python
>>> # dictionary meant to store all target models
>>> models = {}
>>>
>>> # iterate over all enrolments
>>> for current_enrolment in protocol.test_enrolment():
...
...     # target identifier
...     target = current_enrolment['model_id']
...     # the same speaker may be enrolled several times using different target
...     # identifiers. in other words, two different identifiers does not
...     # necessarily not mean two different speakers.
...
...     # path to audio file to use for enrolment
...     audio = current_enrolment['audio']
...
...     # pyannote.core.Timeline containing target speech turns
...     # See http://pyannote.github.io/pyannote-core/structure.html#timeline
...     enrol_with = current_enrolment['enrol_with']
...
...     # this is where enrolment actually happens and model is stored
...     models[target] = enrol(audio, enrol_with)
```

The following pseudo-code shows what the `enrol` function could look like:

```python
>>> def enrol(audio, enrol_with):
...     """Adapt UBM and return GMM"""
...
...     # start by gathering MFCCs
...     mfcc = []
...     for segment in enrol_with:
...         mfcc.append(extract_mfcc(audio, segment.start, segment.end))
...
...     # adapt existing UBM
...     gmm = ubm.adapt(mfcc)
...
...     # return the resulting GMM
...     return gmm
```

### Trial

```python
>>> for current_trial in protocol.test_trial():
...
...     # load target model (precomputed during the enrolment phase)
...     target = current_trial['model_id']
...     model = models[target]
...
...     # path to audio file to use for the trial
...     audio = current_enrolment['audio']

...     # pyannote.core.Segment containing time range where to seek the target
...     # See http://pyannote.github.io/pyannote-core/structure.html#segment
...     try_with = current_enrolment['try_with']
...
...     # this is where speaker spotting actually happens
...     decision = spot(model, audio, try_with)
```

The following pseudo-code shows what the `spot` function could look like:

```python
>>> def spot(model, audio, try_with):
...
...     # perform speaker diarization
...     speakers = speaker_diarization(audio, try_with)
...
...     score = 0.
...     # loop on all speakers
...     for speaker in speakers:
...
...         # gather MFCCs for current speaker
...         mfcc = extract_mfcc(audio, speaker)
...          
...         # compare MFCCs to target model and keep the best score
...         score = max(score, compare(model, mfcc))
...
...     # compare best score to a threshold
...     return score > threshold
```

### Development

`protocol.development()`, `protocol.development_enrolment()`, and
`protocol.development_test()` are also available.

For instance, one could use `protocol.development()` to tune a speaker
diarization module, and `protocol.development_{enrolment|trial}()` to tune
decision thresholds.
