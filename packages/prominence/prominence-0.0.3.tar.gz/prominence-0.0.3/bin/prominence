#!/usr/bin/env python

from __future__ import print_function
import argparse
import base64
from collections import OrderedDict
import errno
import json
import os
import re
import sys
import time
import requests
from prominence import ProminenceTask
from prominence import ProminenceJob
from prominence import ProminenceClient

def elapsed(job):
    """
    Print elapsed job runtime in a nice way
    """
    if 'executionStart' in job['events']:
        if 'completionTime' in job['events']:
            elapsed_time = job['events']['completionTime'] - job['events']['executionStart']
        else:
            elapsed_time = time.time() - job['events']['executionStart']
        days = int(elapsed_time/86400)
        time_fmt = '%H:%M:%S'
        return '%d+%s' % (days, time.strftime(time_fmt, time.gmtime(elapsed_time)))

    return ''

def datetime_format(epoch):
    """
    Convert a unix epoch in a formatted date/time string
    """
    datetime_fmt = '%Y-%m-%dT%H:%M:%S'
    return time.strftime(datetime_fmt, time.gmtime(epoch))

def print_json(content, transform=False, detail=False):
    """
    Print JSON in a nice way
    """
    if transform:
        content = transform_job_list(content, detail)
    print(json.dumps(content, indent=2))

def image_name(name):
    """
    Extract container image name
    """
    if 'http' in name:
        name = os.path.basename(name)
        name = name[:name.find('?')]
    return name

def list_jobs(jobs):
    """
    Print list of jobs
    """

    # Firstly determine column widths
    width_id = 2
    width_created = 19
    width_status = 6
    width_elapsed = 10
    width_container = 5
    width_cmd = 3

    for job in jobs:
        my_cmd = ''
        if 'cmd' in job:
            my_cmd = os.path.basename(job['cmd'])

        width_id_current = len(str(job['id']))
        width_status_current = len(job['status'])
        width_container_current = len(image_name(job['image']))
        width_cmd_current = len((my_cmd))

        if width_id_current > width_id:
            width_id = width_id_current
        if width_status_current > width_status:
            width_status = width_status_current
        if width_container_current > width_container:
            width_container = width_container_current
        if width_cmd_current > width_cmd:
            width_cmd = width_cmd_current

    # Print headings
    print('%s   %s   %s   %s   %s   %s %s' % ('ID'.ljust(width_id),
                                              'CREATED'.ljust(width_created),
                                              'STATUS'.ljust(width_status),
                                              'ELAPSED'.ljust(width_elapsed),
                                              'IMAGE'.ljust(width_container),
                                              'CMD'.ljust(width_cmd),
                                              'ARGS'))

    # Print jobs
    for job in jobs:
        my_cmd = ''
        my_args = ''
        if 'cmd' in job:
            my_cmd = os.path.basename(job['cmd'])
        if 'args' in job:
            my_args = job['args']
        print('%s   %s   %s   %s   %s   %s %s' % (str(job['id']).ljust(width_id),
                                                  datetime_format(job['events']['creationTime']).ljust(width_created),
                                                  job['status'].ljust(width_status),
                                                  elapsed(job).ljust(width_elapsed),
                                                  image_name(job['image']).ljust(width_container),
                                                  my_cmd.ljust(width_cmd),
                                                  my_args))

def transform_job(job, detail):
    """
    Transform a job into the required format for printing
    """
    job_t = OrderedDict()
    job_t['id'] = job['id']
    job_t['status'] = job['status']

    if detail and 'statusReason' in job:
        job_t['statusReason'] = job['statusReason']

    job_t['image'] = job['image']

    if detail:
        job_t['runtime'] = job['runtime']

    if 'cmd' in job:
        job_t['cmd'] = job['cmd']
    if 'args' in job:
        job_t['args'] = job['args']

    if detail:
        job_t['cpus'] = job['cpus']
        job_t['memory'] = job['memory']
        job_t['nodes'] = job['nodes']
        job_t['disk'] = job['disk']
        job_t['wallTimeLimit'] = job['wallTimeLimit']
        if 'env' in job:
            job_t['env'] = job['env']
        if 'labels' in job:
            job_t['labels'] = job['labels']
        if 'artifacts' in job:
            job_t['artifacts'] = job['artifacts']
        if 'inputFiles' in job:
            job_t['inputFiles'] = job['inputFiles']
        if 'outputFiles' in job:
            job_t['outputFiles'] = job['outputFiles']

    events = OrderedDict()
    if 'events' in job:
        if 'creationTime' in job['events']:
            if detail:
                events['creationTime'] = datetime_format(job['events']['creationTime'])
            else:
                events['creationTime'] = job['events']['creationTime']

    if 'executionStart' in job['events']:
        if detail:
            events['executionStart'] = datetime_format(job['events']['executionStart'])
        else:
            events['executionStart'] = job['events']['executionStart']
    if 'completionTime' in job['events']:
        if detail:
            events['completionTime'] = datetime_format(job['events']['completionTime'])
        else:
            events['completionTime'] = job['events']['completionTime']
    job_t['events'] = events

    return job_t

def transform_job_list(result, detail):
    """
    Transform a job list into the required format ordered by id
    """
    jobs = [transform_job(job, detail) for job in result]
    return sorted(jobs, key=lambda k: int(k['id']))

def command_login(args):
    """
    Login to IAM
    """
    data = {}
    data['scope'] = 'openid profile email'
    data['client_id'] = os.environ['PROMINENCE_IAM_CLIENT_ID']

    try:
        request = requests.post(os.environ['PROMINENCE_IAM_URL']+'/devicecode',
                                data=data,
                                timeout=HTTP_TIMEOUT,
                                auth=(os.environ['PROMINENCE_IAM_CLIENT_ID'],
                                      os.environ['PROMINENCE_IAM_CLIENT_SECRET']),
                                allow_redirects=True)
    except requests.exceptions.RequestException:
        print('Error: Cannot connect to IAM server')
        exit(1)

    device_code_response = request.json()

    print('To login, use a web browser to open the page %s and enter the code %s when requested' % (device_code_response['verification_uri'], device_code_response['user_code']))

    data = {}
    data['grant_type'] = 'urn:ietf:params:oauth:grant-type:device_code'
    data['device_code'] = device_code_response['device_code']

    # Wait for the user to authenticate
    current_time = time.time()
    authenticated = False
    while time.time() < current_time + int(device_code_response['expires_in']) and not authenticated:
        time.sleep(5)
        try:
            request = requests.post(os.environ['PROMINENCE_IAM_URL']+'/token',
                                    data=data,
                                    timeout=HTTP_TIMEOUT,
                                    auth=(os.environ['PROMINENCE_IAM_CLIENT_ID'],
                                          os.environ['PROMINENCE_IAM_CLIENT_SECRET']),
                                    allow_redirects=True)
        except requests.exceptions.RequestException:
            print('Error: Cannot connect to IAM server')
            exit(1)
        if request.status_code == 200:
            authenticated = True
            with open(os.path.expanduser('~/.prominence'), 'w') as token_file:
                json.dump(request.json(), token_file)
            os.chmod(os.path.expanduser('~/.prominence'), 384)
            print('Authentication successful')
            exit(0)

    if not authenticated:
        print('Error: Authentication failed')
        exit(1)

    try:
        with open(args.file) as json_file:
            data = json.load(json_file)
    except IOError as err:
        print('Error: %s' % err)
        exit(1)
    except ValueError as err:
        print('Error: %s' % err)
        exit(1)

def get_token():
    """
    Load saved token
    """
    if os.path.isfile(os.path.expanduser('~/.prominence')):
        try:
            with open(os.path.expanduser('~/.prominence')) as json_data:
                data = json.load(json_data)
        except IOError as err:
            print('Error: %s' % err)
            exit(1)
        except ValueError as err:
            print('Error: %s' % err)
            exit(1)

        if 'access_token' in data:
            return data['access_token']
        else:
            print('Error: the saved token file does not contain access_token')
            exit(1)
    return None

def command_list(args):
    """
    List running/idle jobs or completed jobs
    """
    completed = False
    if args.completed:
        completed = True
    all = False
    if args.all:
        all = True
    num = None
    if args.num:
        num = args.num
    constraint = None
    if args.constraint:
        constraint = args.constraint

    client = ProminenceClient(url=URL, token=TOKEN)
    response = client.list(completed, all, num, constraint)
    if response.return_code == 0:
        list_jobs(transform_job_list(response.data, False))
        exit(0)
    else:
        print('Error: %s' % response.data['error'])
        exit(1)

def command_describe(args):
    """
    Describe specific job
    """
    completed = False
    if args.completed:
        completed = True

    client = ProminenceClient(url=URL, token=TOKEN)
    response = client.describe(args.id, completed)
    if response.return_code != 0:
        print('Error: %s' % response.data['error'])
        exit(1)
    print_json(response.data, transform=True, detail=True)
    exit(0)

def command_download(args):
    """
    Download output files
    """
    client = ProminenceClient(url=URL, token=TOKEN)

    if args.constraint:
        response = client.list(False, True, 0, args.constraint)
    else:
        if not args.id:
            print('Error: A job id must be given if a constraint if not specified')
            exit(1)
        response = client.describe(args.id, True)

    if response.return_code != 0:
        print('Error: %s' % response.data['error'])
        exit(1)

    jobs = response.data
    for job in jobs:
        if 'outputFiles' in job and job['status'] == 'completed':
            # Create per-job directory if necessary & set path for files
            path = './'
            if args.dir:
                path = './%d/' % job['id']
                try:
                    os.mkdir(path)
                except OSError as exc:
                    if exc.errno != errno.EEXIST:
                        print('WARNING: skipping job %d as job directory cannot be created' % job['id'])
                        continue
                    else:
                        pass

            for pair in job['outputFiles']:
                file_name = os.path.basename(pair['name'])
                url = pair['url']

                if os.path.isfile(path + file_name) and not args.force:
                    print('WARNING: skipping "%s" from job %d as file already exists and force option not specified' % (file_name, job['id']))
                    continue

                response = requests.get(url, stream=True)
                total_length = response.headers.get('content-length')

                if response.status_code != 200:
                    print('WARNING: skipping "%s" from job %d as it does not exist' % (file_name, job['id']))
                    continue

                with open(path + file_name, 'wb') as file_download:
                    print('Downloading file "%s" from job %d' % (file_name, job['id']))
                    if total_length is None:
                        file_download.write(response.content)
                    else:
                        downloaded = 0
                        total_length = int(total_length)
                        for data in response.iter_content(chunk_size=4096):
                            downloaded += len(data)
                            file_download.write(data)
                            done = int(50 * downloaded / total_length)
                            sys.stdout.write("\r[%s%s]" % ('=' * done, ' ' * (50-done)))
                            sys.stdout.flush()
                print('')

def command_delete(args):
    """
    Delete a job
    """
    client = ProminenceClient(url=URL, token=TOKEN)
    response = client.delete(args.id)
    if response.return_code != 0:
        print('Error: %s' % response.data['error'])
        exit(1)
    print('Success')
    exit(0)

def command_stdout(args):
    """
    Get standard output for a specific job
    """
    client = ProminenceClient(url=URL, token=TOKEN)
    response = client.stdout(args.id)
    if response.return_code != 0:
        print('Error: %s' % response.data['error'])
        exit(1)
    print(response.data)
    exit(0)

def command_stderr(args):
    """
    Get standard error for a specific job
    """
    client = ProminenceClient(url=URL, token=TOKEN)
    response = client.stderr(args.id)
    if response.return_code != 0:
        print('Error: %s' % response.data['error'])
        exit(1)
    print(response.data)
    exit(0)

def command_create(args):
    """
    Create a job from a JSON file
    """
    try:
        with open(args.file) as json_file:
            data = json.load(json_file)
    except IOError as err:
        print('Error: %s' % err)
        exit(1)
    except ValueError as err:
        print('Error: %s' % err)
        exit(1)

    client = ProminenceClient(url=URL, token=TOKEN)
    response = client.create_from_json(data)
    if response.return_code == 0:
        if 'id' in response.data:
            print('Job created with id %d' % response.data['id'])
        exit(0)
    else:
        if 'error' in response.data:
            print('Error: %s' % response.data['error'])
    exit(1)

def command_run(args):
    """
    Run a job
    """
    job = ProminenceJob()
    task = ProminenceTask()

    task.image = args.image
    job.memory = args.memory
    job.cpus = args.cpus
    job.nodes = args.nodes
    job.disk = args.disk
    job.instances = args.instances

    # Walltine limit
    job.walltime = args.walltime

    # Container runtime - use singularity by default but use udocker if the user has
    # specified a tarball using a URL
    if args.runtime:
        task.runtime = args.runtime
    else:
        if re.match(r'^http', args.image) and '.tar' in args.image:
            task.runtime = 'udocker'
        else:
            task.runtime = 'singularity'

    # Job type
    if args.mpi:
        task.type = 'mpi'
    else:
        task.type = 'basic'

    # Force MPI if more than one node has been requested
    if job.nodes > 1:
        task.type = 'mpi'

    # Set MPI version for MPI jobs
    if args.mpiversion and task.type == 'mpi':
        task.mpi_version = args.mpiversion

    # Extract command and arguments from command string
    if args.command:
        if ' ' in args.command:
            task.cmd = args.command.split(" ", 1)[0]
            task.args = args.command.split(" ", 1)[1]
        else:
            task.cmd = args.command

    # Output files
    if args.outputfile:
        job.output_files = args.outputfile

    # Files to be fetched
    if args.artifact:
        job.artifacts = args.artifact

    # Files to be uploaded
    if args.inputfile:
        inputs = []
        for filename in args.inputfile:
            if os.path.isfile(filename):
                if os.path.getsize(filename) < 1000000:
                    with open(filename, 'rb') as input_file:
                        inputs.append({'filename':filename, 'content':base64.b64encode(input_file.read())})
                else:
                    print('Error: Input file size too large')
                    exit(1)
        job.inputs = inputs

    # Environment variables
    if args.env:
        env = {}
        for pair in args.env:
            if '=' in pair:
                items = pair.split('=')
                env[items[0]] = items[1]
        task.env = env

    # Metadata
    if args.label:
        job.labels = args.label

    # Constraints
    if args.constraints:
        job.constraints = json.loads(args.constraints)

    # Add task to job
    job.tasks = [task]

    # Print JSON description of job if requested
    if args.dryrun:
        print_json(job.to_json())
        exit(0)

    client = ProminenceClient(url=URL, token=TOKEN)
    response = client.create(job)
    if response.return_code == 0:
        if 'id' in response.data:
            print('Job created with id %d' % response.data['id'])
        exit(0)
    else:
        if 'error' in response.data:
            print('Error: %s' % response.data['error'])
    exit(1)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Prominence - \
                                     run jobs in containers across clouds')
    subparsers = parser.add_subparsers(help='sub-command help')

    # Create the parser for the "login" command
    parser_login = subparsers.add_parser('login', help='Login')
    parser_login.set_defaults(func=command_login)

    # Create the parser for the "create" command
    parser_create = subparsers.add_parser('create', help='Create a job from a JSON file')
    parser_create.add_argument('file', help='JSON file')
    parser_create.set_defaults(func=command_create)

    # Create the parser for the "run" command
    parser_run = subparsers.add_parser('run', help='Run a job')
    parser_run.add_argument('--memory', dest='memory', default=1, type=int,
                            help='Memory in GB per node.')
    parser_run.add_argument('--cpus', dest='cpus', default=1, type=int,
                            help='Cores per node.')
    parser_run.add_argument('--nodes', dest='nodes', default=1, type=int,
                            help='Number of nodes. MPI will be assumed if more \
                                  than one node is specified.')
    parser_run.add_argument('--instances', dest='instances', default=1, type=int,
                            help='Number of instances of this job.')
    parser_run.add_argument('--disk', dest='disk', default=10, type=int,
                            help='Size of disk containing the job\'s scratch directory. For \
                                  multi-node jobs it will be shared across each of the nodes. \
                                  By default a 10 GB disk will be used.')
    parser_run.add_argument('--walltime', dest='walltime', type=int,
                            help='Walltime limit in minutes. If the job is still running after \
                                  this time it will be killed.')
    parser_run.add_argument('--mpi', dest='mpi', default=False, action='store_true',
                            help="Specify that this is an MPI job.")
    parser_run.add_argument('--mpi-version', dest='mpiversion', default='1.10.7',
                            choices=['1.10.7', '2.1.1', '3.0.2', '3.1.0'],
                            help='MPI version. The default is 1.10.7.')
    parser_run.add_argument('--artifact', dest='artifact', action='append',
                            help='A URL to be transferred to the job. Archives will be \
                                  automatically unpacked/extracted. \
                                  This option can be specified multiple times.')
    parser_run.add_argument('--input', dest='inputfile', action='append',
                            help='Full path to a file on the current host to be \
                                  uploaded and made available to the job. This option \
                                  can be specified multiple times to set multiple output files.')
    parser_run.add_argument('--output', dest="outputfile", action='append',
                            help='An output file to be copied to cloud storage. This option \
                                  can be specified multiple times to set multiple output files.')
    parser_run.add_argument('--env', dest='env', action='append',
                            help='Specify environment variables in the form name=value. \
                                  This option can be specified multiple times to set \
                                  multiple environment variables.')
    parser_run.add_argument('--label', dest='label', action='append',
                            help='Set metadata in the form key=value. This option can \
                                  be specified multiple times to set multiple labels.')
    parser_run.add_argument('--runtime', dest='runtime',
                            choices=['singularity', 'udocker'],
                            help='Container runtime, either singularity or udocker. The default \
                                  is singularity.')
    parser_run.add_argument('--constraints', dest='constraints',
                            help='Specify constraints')
    parser_run.add_argument('--dry-run', dest='dryrun', default=False, action='store_true',
                            help='Print json to stdout but do not actually create job.')
    parser_run.add_argument('image', help='Container image')
    parser_run.add_argument('command', nargs='?',
                            help='Command to run in the container. If you need to specify \
                                  arguments, put the combined command and arguments inside quotes.')
    parser_run.set_defaults(func=command_run)

    # Create the parser for the "list" command
    parser_list = subparsers.add_parser('list', help='List jobs')
    parser_list.add_argument('--completed', dest='completed', default=False,
                             help='List completed jobs', action='store_true')
    parser_list.add_argument('--num', dest='num', default=1, type=int,
                             help='Number of completed jobs to return')
    parser_list.add_argument('--constraint', dest='constraint', action='append',
                             help='Constraint of the form key=value')
    parser_list.add_argument('--all', dest='all', default=False,
                             help='List jobs in all states', action='store_true')
    parser_list.set_defaults(func=command_list)

    # Create the parser for the "describe" command
    parser_describe = subparsers.add_parser('describe', help='Describe a job')
    parser_describe.add_argument('id', help='Job id', type=int)
    parser_describe.add_argument('--completed', dest='completed', default=False,
                                 help='Describe a job in the completed state', action='store_true')
    parser_describe.set_defaults(func=command_describe)

    # Create the parser for the "delete" command
    parser_delete = subparsers.add_parser('delete', help='Delete a job')
    parser_delete.add_argument('id', help='Job id', type=int)
    parser_delete.set_defaults(func=command_delete)

    # Create the parser for the "download" command
    parser_download = subparsers.add_parser('download', help='Download output files from a completed job')
    parser_download.add_argument('--constraint', dest='constraint', action='append',
                                 help='Constraint of the form key=value')
    parser_download.add_argument('--force', dest='force', default=False,
                                 help='Force overwrite of existing file', action='store_true')
    parser_download.add_argument('--dir', dest='dir', default=False,
                                 help='Save output files in a directory named by the job id', action='store_true')
    parser_download.add_argument('id', help='Job id', type=int, nargs='?')
    parser_download.set_defaults(func=command_download)

    # Create the parser for the "stdout" command
    parser_stdout = subparsers.add_parser('stdout', help='Get standard output from a running or completed job')
    parser_stdout.add_argument('id', help='Job id', type=int)
    parser_stdout.set_defaults(func=command_stdout)

    # Create the parser for the "stderr" command
    parser_stderr = subparsers.add_parser('stderr', help='Get standard error from a running or completed job')
    parser_stderr.add_argument('id', help='Job id', type=int)
    parser_stderr.set_defaults(func=command_stderr)

    # Print help if necessary
    if len(sys.argv) < 2:
        parser.print_help(sys.stderr)
        exit(1)

    # Parse the arguments & run the required function if necessary
    args = parser.parse_args()

    # Authentication
    if 'PROMINENCE_IAM_URL' not in os.environ:
        print('Error: environment variable PROMINENCE_IAM_URL is not set')
        exit(1)

    if 'PROMINENCE_IAM_CLIENT_ID' not in os.environ:
        print('Error: environment variable PROMINENCE_IAM_CLIENT_ID is not set')
        exit(1)

    if 'PROMINENCE_IAM_CLIENT_SECRET' not in os.environ:
        print('Error: environment variable PROMINENCE_IAM_CLIENT_SECRET is not set')
        exit(1)

    # Get existing token if possible when necessary
    if sys.argv[1] != 'login':
        token = get_token()
        if token:
            HEADERS = {"Authorization":"Bearer %s" % token}
            TOKEN = token
        else:
            print('Error: Authentication required')
            exit(1)

    # Get URL for PROMINENCE service
    if 'PROMINENCE_URL' in os.environ:
        URL = os.environ['PROMINENCE_URL']
    else:
        print('Error: Environment variable PROMINENCE_URL is not set')
        exit(1)

    HTTP_TIMEOUT = 20

    # Run
    args.func(args)
